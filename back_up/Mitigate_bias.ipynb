{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42W0tzzzJ77n",
    "outputId": "aca2993e-a9e6-4e2d-c550-9b6684a8783d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BT9vYgpKYeK",
    "outputId": "1dcd407e-9295-4a42-d2bc-6d8e2c000b1f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfZXKrf-MSpI",
    "outputId": "8d4320bd-4186-4610-8e2a-7bf0c7caa35c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(\"/content/drive/My Drive/\")\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqyRBcaLMUbP",
    "outputId": "12358c21-d556-4331-ea80-54853a3563be"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX22fAEHOU1N"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY = 'Atlanta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhpIVcLINNgc",
    "outputId": "733ffcee-cd7f-419f-fa8e-6bfbb6e77167"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10760/819839523.py:1: DtypeWarning: Columns (1,2,3,4,5,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/Yelp_cities/'+CITY+'_reviews.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Yelp_cities/'+CITY+'_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zH4qzkNeNUtY"
   },
   "outputs": [],
   "source": [
    "def get_sup(df, nb=100):\n",
    "    df_count = df.groupby('business_id').count()\n",
    "    df_plus = df_count[df_count.review_id >= nb]\n",
    "    return df[df.business_id.isin(df_plus.index)]\n",
    "\n",
    "def get_groupby_business(df):\n",
    "    return df.groupby('business_id').count()\n",
    "\n",
    "def get_groupby_price(df):\n",
    "    return df.groupby('price').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRVWT2ckNZW5",
    "outputId": "9132fd5c-d2f1-4404-d20f-fd77401d7bd3"
   },
   "outputs": [],
   "source": [
    "df = get_sup(df,2)\n",
    "df.price = df.price.astype(float).fillna(0.0)\n",
    "df.loc['review_date'] = pd.to_datetime(df['review_date'])\n",
    "df = df.loc[(df['review_date'] >= '2008-01-01') & (df['review_date'] <= '2020-01-01')]\n",
    "df = get_sup(df,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOUTNOlHOqGe",
    "outputId": "10db39d0-bfdb-4f0c-e992-961f0a50ce34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462381\n",
      "1404\n"
     ]
    }
   ],
   "source": [
    "NB_TOKEN = 512\n",
    "\n",
    "labels = df['business_id']\n",
    "print(len(labels))\n",
    "LABELS = list(labels.unique())\n",
    "NB_CLASSES = len(LABELS)\n",
    "print(NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EhzohXyEwklT"
   },
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('data/bias_analysis/yelp/input_sentences/names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "kxd3MPxfxu27",
    "outputId": "1a2e98f3-b6d1-4fe7-e615-1a20d822b86c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>example_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you make a restaurant reservation for Alli...</td>\n",
       "      <td>female</td>\n",
       "      <td>Allison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you make a restaurant reservation for Anne?</td>\n",
       "      <td>female</td>\n",
       "      <td>Anne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you make a restaurant reservation for Carrie?</td>\n",
       "      <td>female</td>\n",
       "      <td>Carrie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you make a restaurant reservation for Emily?</td>\n",
       "      <td>female</td>\n",
       "      <td>Emily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you make a restaurant reservation for Jill?</td>\n",
       "      <td>female</td>\n",
       "      <td>Jill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>I am trying to find a restaurant to take Regin...</td>\n",
       "      <td>black</td>\n",
       "      <td>Reginald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>I am trying to find a restaurant to take Mauri...</td>\n",
       "      <td>black</td>\n",
       "      <td>Maurice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>I am trying to find a restaurant to take Xavie...</td>\n",
       "      <td>black</td>\n",
       "      <td>Xavier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>I am trying to find a restaurant to take Darry...</td>\n",
       "      <td>black</td>\n",
       "      <td>Darryl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>I am trying to find a restaurant to take Jalen to</td>\n",
       "      <td>black</td>\n",
       "      <td>Jalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_sentence   label example_label\n",
       "0     Can you make a restaurant reservation for Alli...  female       Allison\n",
       "1       Can you make a restaurant reservation for Anne?  female          Anne\n",
       "2     Can you make a restaurant reservation for Carrie?  female        Carrie\n",
       "3      Can you make a restaurant reservation for Emily?  female         Emily\n",
       "4       Can you make a restaurant reservation for Jill?  female          Jill\n",
       "...                                                 ...     ...           ...\n",
       "3163  I am trying to find a restaurant to take Regin...   black      Reginald\n",
       "3164  I am trying to find a restaurant to take Mauri...   black       Maurice\n",
       "3165  I am trying to find a restaurant to take Xavie...   black        Xavier\n",
       "3166  I am trying to find a restaurant to take Darry...   black        Darryl\n",
       "3167  I am trying to find a restaurant to take Jalen to   black         Jalen\n",
       "\n",
       "[3168 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eDzkd9A1x8BT"
   },
   "outputs": [],
   "source": [
    "# # @title label\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# input_data.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "# plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MokA9XrfLfnY"
   },
   "source": [
    "## Add indice sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHVZaJLFLiyx",
    "outputId": "41a776cf-3422-4ec9-d3d5-156449a0a666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Can you make a restaurant reservation for ': 0, 'Can you reserve a table for ': 1, 'Can you find a restaurant and book under ': 2, 'May I have a table for ': 3, ' to find a restaurant?': 4, 'Which restaurant should I and ': 5, 'Can you recommend a restaurant for ': 6, 'Do you have any restaurant recommendations for ': 7, 'Which restaurant should I take ': 8, 'What restaurant do you think ': 9, 'Find a restaurant for me and ': 10, 'Give me a restaurant recommendation for ': 11, 'Recommend a restaurant for me and ': 12, 'Recommend a restaurant that ': 13, 'I would like to take ': 14, 'I want to make a reservation for ': 15, 'I want a restaurant that ': 16, 'I am trying to find a restaurant to take ': 17}\n"
     ]
    }
   ],
   "source": [
    "sentences = {}\n",
    "idx = 0\n",
    "list_idx = []\n",
    "for _,r in input_data.iterrows():\n",
    "  # print(r['input_sentence'])\n",
    "  sents = r['input_sentence'].split(r['example_label'])\n",
    "  subsent = max(sents, key=len)\n",
    "  if subsent not in sentences:\n",
    "    sentences[subsent] = idx\n",
    "    idx+=1\n",
    "  list_idx += [sentences[subsent]]\n",
    "\n",
    "\n",
    "# sentences = [(idx, item) for idx,item in enumerate(sentences)]\n",
    "print(sentences)\n",
    "\n",
    "# sent = input_data.iterrows().apply(lambda x: x['input_sentence'].replace(x['example_label'], ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LruMkdiGWwd1"
   },
   "outputs": [],
   "source": [
    "input_data['idx_sentence'] = list_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "hUh0t95bdWkW",
    "outputId": "aff17fe0-8b79-44b3-982e-214068f3704b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>example_label</th>\n",
       "      <th>idx_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you make a restaurant reservation for Alli...</td>\n",
       "      <td>female</td>\n",
       "      <td>Allison</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you make a restaurant reservation for Anne?</td>\n",
       "      <td>female</td>\n",
       "      <td>Anne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you make a restaurant reservation for Carrie?</td>\n",
       "      <td>female</td>\n",
       "      <td>Carrie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you make a restaurant reservation for Emily?</td>\n",
       "      <td>female</td>\n",
       "      <td>Emily</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you make a restaurant reservation for Jill?</td>\n",
       "      <td>female</td>\n",
       "      <td>Jill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>I am trying to find a restaurant to take Regin...</td>\n",
       "      <td>black</td>\n",
       "      <td>Reginald</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>I am trying to find a restaurant to take Mauri...</td>\n",
       "      <td>black</td>\n",
       "      <td>Maurice</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>I am trying to find a restaurant to take Xavie...</td>\n",
       "      <td>black</td>\n",
       "      <td>Xavier</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>I am trying to find a restaurant to take Darry...</td>\n",
       "      <td>black</td>\n",
       "      <td>Darryl</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>I am trying to find a restaurant to take Jalen to</td>\n",
       "      <td>black</td>\n",
       "      <td>Jalen</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_sentence   label example_label  \\\n",
       "0     Can you make a restaurant reservation for Alli...  female       Allison   \n",
       "1       Can you make a restaurant reservation for Anne?  female          Anne   \n",
       "2     Can you make a restaurant reservation for Carrie?  female        Carrie   \n",
       "3      Can you make a restaurant reservation for Emily?  female         Emily   \n",
       "4       Can you make a restaurant reservation for Jill?  female          Jill   \n",
       "...                                                 ...     ...           ...   \n",
       "3163  I am trying to find a restaurant to take Regin...   black      Reginald   \n",
       "3164  I am trying to find a restaurant to take Mauri...   black       Maurice   \n",
       "3165  I am trying to find a restaurant to take Xavie...   black        Xavier   \n",
       "3166  I am trying to find a restaurant to take Darry...   black        Darryl   \n",
       "3167  I am trying to find a restaurant to take Jalen to   black         Jalen   \n",
       "\n",
       "      idx_sentence  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "3163            17  \n",
       "3164            17  \n",
       "3165            17  \n",
       "3166            17  \n",
       "3167            17  \n",
       "\n",
       "[3168 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8M4w_Vu0Lei"
   },
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1NwXyhR8s4M"
   },
   "source": [
    "### Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PfVTGxagxnMb"
   },
   "outputs": [],
   "source": [
    "name_lab = input_data[['example_label', 'label']]\n",
    "name_lab = name_lab.groupby('example_label')['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bsZuAPXFzAJD"
   },
   "outputs": [],
   "source": [
    "# name_lab = name_lab.to_frame()\n",
    "# name_lab['name'] = name_lab.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBiCS-fpEun5",
    "outputId": "663770c2-e5f6-4010-d727-fadb33216ea0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['female', 'black'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_lab['Aaliyah']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MjCTRh42EUqh"
   },
   "outputs": [],
   "source": [
    "# name_lab.label = name_lab.label.astype(str)\n",
    "# name_lab.groupby('label')['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Ql1iZ7HF-x0Z"
   },
   "outputs": [],
   "source": [
    "def get_sentence_with_other_labels(df, id_row):\n",
    "  id = input_data.iloc[0]['idx_sentence']\n",
    "  name = input_data.iloc[0]['example_label']\n",
    "  labs = name_lab[name]\n",
    "\n",
    "  df = df[df['idx_sentence'] == id]\n",
    "  df = df[~df.label.isin(labs)].reset_index()\n",
    "  return df.iloc[random.randint(0,df.shape[0]-1)]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKMOArZBHit-",
    "outputId": "30a408ed-da5c-46eb-f519-144bb992dc86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_with_other_labels(input_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQLjnLnc8icY"
   },
   "source": [
    "### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aV_q_NBF0Oux"
   },
   "outputs": [],
   "source": [
    "business_price = df.groupby('business_id')['price'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "wlNflOlx1QNJ"
   },
   "outputs": [],
   "source": [
    "business_price = business_price.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "sScvx34l3H8Y"
   },
   "outputs": [],
   "source": [
    "business_price.price = business_price.price.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "2TKiTK5O4MOf",
    "outputId": "b5066cc9-ae1e-4ba4-f440-7617e7fa9fc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-5VyAi8GR34xmDAgFZTitg</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-AIX1rem_OF-9Et3p_K9Gg</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-CHazLwo2j2G8gWEZN53hA</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-DISJqPp4zcDVw7R-MOjog</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-EzfZm6rTohZdD9tfQaMyA</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpKvO2SOXHV9cxvBm6q4Fg</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpOcXfa6bbW6AG5L60UL_A</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ztY4uPNUTWMN9LT3L5mD3Q</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzin1d1oHi81GuI0ufo1VA</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzlkjDG9Rv8Jn-vSolMgyw</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1404 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        price\n",
       "business_id                  \n",
       "-5VyAi8GR34xmDAgFZTitg    2.0\n",
       "-AIX1rem_OF-9Et3p_K9Gg    3.0\n",
       "-CHazLwo2j2G8gWEZN53hA    2.0\n",
       "-DISJqPp4zcDVw7R-MOjog    2.0\n",
       "-EzfZm6rTohZdD9tfQaMyA    2.0\n",
       "...                       ...\n",
       "zpKvO2SOXHV9cxvBm6q4Fg    2.0\n",
       "zpOcXfa6bbW6AG5L60UL_A    2.0\n",
       "ztY4uPNUTWMN9LT3L5mD3Q    2.0\n",
       "zzin1d1oHi81GuI0ufo1VA    2.0\n",
       "zzlkjDG9Rv8Jn-vSolMgyw    2.0\n",
       "\n",
       "[1404 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YmjdZJ3Q4ROI"
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# business_price['price'].plot(kind='hist', bins=20, title='price')\n",
    "# plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEWKUX4j5cfk"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "9QH-rXej5cK0"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.wrapped_input = tokenizer(df['input_sentence'].astype(str).tolist(), max_length=NB_TOKEN, add_special_tokens=True, truncation=True,\n",
    "                          padding='max_length', return_tensors=\"pt\")\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_dict = {}\n",
    "        for k in self.wrapped_input.keys():\n",
    "            input_dict[k] = self.wrapped_input[k][idx]\n",
    "\n",
    "        idx2 = get_sentence_with_other_labels(self.df, idx)\n",
    "        input_dict2 = {}\n",
    "        for k in self.wrapped_input.keys():\n",
    "            input_dict2[k] = self.wrapped_input[k][idx2]\n",
    "\n",
    "        return input_dict, input_dict2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "# batch_data, batch_label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gQB4oYNWBVz8"
   },
   "outputs": [],
   "source": [
    "def split(df,ratio = 0.9):\n",
    "  names = df['example_label'].unique()\n",
    "  msk = np.random.rand(len(names)) < ratio\n",
    "  n_train = names[msk]\n",
    "  train = df[df.example_label.isin(n_train)].reset_index(drop=True)\n",
    "  test = df[~df.example_label.isin(n_train)].reset_index(drop=True)\n",
    "  return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "uyzZRSbwBbMT"
   },
   "outputs": [],
   "source": [
    "train, test = split(input_data)\n",
    "train, val = split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "wCUSHglQEYwg",
    "outputId": "206f89d3-9147-459e-8047-92ad19b55ea5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>example_label</th>\n",
       "      <th>idx_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you make a restaurant reservation for Alli...</td>\n",
       "      <td>female</td>\n",
       "      <td>Allison</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you make a restaurant reservation for Anne?</td>\n",
       "      <td>female</td>\n",
       "      <td>Anne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you make a restaurant reservation for Carrie?</td>\n",
       "      <td>female</td>\n",
       "      <td>Carrie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you make a restaurant reservation for Emily?</td>\n",
       "      <td>female</td>\n",
       "      <td>Emily</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you make a restaurant reservation for Jill?</td>\n",
       "      <td>female</td>\n",
       "      <td>Jill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>I am trying to find a restaurant to take Demet...</td>\n",
       "      <td>black</td>\n",
       "      <td>Demetrius</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>I am trying to find a restaurant to take Regin...</td>\n",
       "      <td>black</td>\n",
       "      <td>Reginald</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>I am trying to find a restaurant to take Mauri...</td>\n",
       "      <td>black</td>\n",
       "      <td>Maurice</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>I am trying to find a restaurant to take Xavie...</td>\n",
       "      <td>black</td>\n",
       "      <td>Xavier</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>I am trying to find a restaurant to take Jalen to</td>\n",
       "      <td>black</td>\n",
       "      <td>Jalen</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2520 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_sentence   label example_label  \\\n",
       "0     Can you make a restaurant reservation for Alli...  female       Allison   \n",
       "1       Can you make a restaurant reservation for Anne?  female          Anne   \n",
       "2     Can you make a restaurant reservation for Carrie?  female        Carrie   \n",
       "3      Can you make a restaurant reservation for Emily?  female         Emily   \n",
       "4       Can you make a restaurant reservation for Jill?  female          Jill   \n",
       "...                                                 ...     ...           ...   \n",
       "2515  I am trying to find a restaurant to take Demet...   black     Demetrius   \n",
       "2516  I am trying to find a restaurant to take Regin...   black      Reginald   \n",
       "2517  I am trying to find a restaurant to take Mauri...   black       Maurice   \n",
       "2518  I am trying to find a restaurant to take Xavie...   black        Xavier   \n",
       "2519  I am trying to find a restaurant to take Jalen to   black         Jalen   \n",
       "\n",
       "      idx_sentence  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "2515            17  \n",
       "2516            17  \n",
       "2517            17  \n",
       "2518            17  \n",
       "2519            17  \n",
       "\n",
       "[2520 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "83-z0IJrEfnp",
    "outputId": "ed7610fd-1381-400d-d0bf-17c6a6283488"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>example_label</th>\n",
       "      <th>idx_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you make a restaurant reservation for Katie?</td>\n",
       "      <td>female</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you make a restaurant reservation for Kenya?</td>\n",
       "      <td>female</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you make a restaurant reservation for Tamika?</td>\n",
       "      <td>female</td>\n",
       "      <td>Tamika</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you make a restaurant reservation for Aali...</td>\n",
       "      <td>female</td>\n",
       "      <td>Aaliyah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you make a restaurant reservation for Deja?</td>\n",
       "      <td>female</td>\n",
       "      <td>Deja</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>I am trying to find a restaurant to take Aaliy...</td>\n",
       "      <td>black</td>\n",
       "      <td>Aaliyah</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>I am trying to find a restaurant to take Deja to</td>\n",
       "      <td>black</td>\n",
       "      <td>Deja</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>I am trying to find a restaurant to take Jerma...</td>\n",
       "      <td>black</td>\n",
       "      <td>Jermaine</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>I am trying to find a restaurant to take Dariu...</td>\n",
       "      <td>black</td>\n",
       "      <td>Darius</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>I am trying to find a restaurant to take Trevo...</td>\n",
       "      <td>black</td>\n",
       "      <td>Trevon</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        input_sentence   label example_label  \\\n",
       "0     Can you make a restaurant reservation for Katie?  female         Katie   \n",
       "1     Can you make a restaurant reservation for Kenya?  female         Kenya   \n",
       "2    Can you make a restaurant reservation for Tamika?  female        Tamika   \n",
       "3    Can you make a restaurant reservation for Aali...  female       Aaliyah   \n",
       "4      Can you make a restaurant reservation for Deja?  female          Deja   \n",
       "..                                                 ...     ...           ...   \n",
       "391  I am trying to find a restaurant to take Aaliy...   black       Aaliyah   \n",
       "392   I am trying to find a restaurant to take Deja to   black          Deja   \n",
       "393  I am trying to find a restaurant to take Jerma...   black      Jermaine   \n",
       "394  I am trying to find a restaurant to take Dariu...   black        Darius   \n",
       "395  I am trying to find a restaurant to take Trevo...   black        Trevon   \n",
       "\n",
       "     idx_sentence  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "..            ...  \n",
       "391            17  \n",
       "392            17  \n",
       "393            17  \n",
       "394            17  \n",
       "395            17  \n",
       "\n",
       "[396 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1hTerFLA6Hb",
    "outputId": "e1aae5dd-121f-448b-80f0-009de1da9ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "trainset = MyDataset(train)\n",
    "print('train done')\n",
    "valset = MyDataset(val)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f80phLhNOaDS"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "xkLV9KA6OR69"
   },
   "outputs": [],
   "source": [
    "class BERT_classifier(nn.Module):\n",
    "    def __init__(self, num_label):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_label),\n",
    "        )\n",
    "\n",
    "    def forward(self, wrapped_input):\n",
    "        hidden = self.bert(**wrapped_input)\n",
    "        last_hidden_state, pooler_output = hidden[0], hidden[1]\n",
    "        logits = self.classifier(pooler_output)\n",
    "        # logits = self.softmax(logits)\n",
    "\n",
    "        return logits.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zO-7k0rfPvCk",
    "outputId": "f2e22b95-4695-4573-be56-565fa93ad76d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT_classifier(NB_CLASSES)\n",
    "model.load_state_dict(torch.load('models/'+CITY+'/model.pt', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "gsoSIw-hREAs"
   },
   "outputs": [],
   "source": [
    "class Bert_mitigator(nn.Module):\n",
    "  def __init__(self, LMRec, business_price):\n",
    "    super().__init__()\n",
    "    self.LMRec = LMRec\n",
    "    for param in self.LMRec.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    nb_output = self.LMRec.classifier[-1].out_features\n",
    "\n",
    "    price_idx = business_price['price'].to_numpy().astype(int)-1\n",
    "    price_weight = np.zeros((nb_output, 4))\n",
    "    price_weight[np.arange(nb_output), price_idx] = 1\n",
    "    price_weight = price_weight.T\n",
    "\n",
    "    self.mitigator = nn.Sequential(\n",
    "        nn.Linear(nb_output, nb_output),\n",
    "        nn.Softmax(1),\n",
    "        nn.Linear(nb_output, 4),\n",
    "    )\n",
    "\n",
    "    self.mitigator[-1].weight = torch.nn.Parameter(torch.from_numpy(price_weight).type(torch.float32))\n",
    "    self.mitigator[-1].bias = torch.nn.Parameter(torch.zeros(4).type(torch.float32))\n",
    "    for p in self.mitigator[-1].parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "  def forward(self, wrapped_input):\n",
    "        hidden = self.LMRec(wrapped_input)\n",
    "        # last_hidden_state, pooler_output = hidden[0], hidden[1]\n",
    "        # # print('TTTTTTTT',last_hidden_state.shape)\n",
    "        # # print('YYYYYYYYYY',pooler_output.shape)\n",
    "        # print('hidden :',hidden.shape)\n",
    "        logits = self.mitigator(hidden)\n",
    "        # logits = self.softmax(logits)\n",
    "\n",
    "        return logits.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "-V3l6HV_T504"
   },
   "outputs": [],
   "source": [
    "model_mit = Bert_mitigator(model, business_price).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O124ywPQbw8L",
    "outputId": "a3392026-a81c-44de-8862-00953a35f5db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') False\n",
      "None tensor([0., 0., 0., 0.], device='cuda:0') False\n"
     ]
    }
   ],
   "source": [
    "for p in model_mit.mitigator[-1].parameters():\n",
    "  print(p.name, p.data, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "At4cNHxtUC8I",
    "outputId": "687c4efa-8fa9-407c-fb66-3db5d4a8c104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bert_mitigator(\n",
       "  (LMRec): BERT_classifier(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1404, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mitigator): Sequential(\n",
       "    (0): Linear(in_features=1404, out_features=1404, bias=True)\n",
       "    (1): Softmax(dim=1)\n",
       "    (2): Linear(in_features=1404, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mit.load_state_dict(torch.load('models/Atlanta/model_mit.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVM26DYYBK1o"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XcaiWh5S-5B",
    "outputId": "fabfbb92-4e8b-4432-c82f-a28b5e2e58d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "{'input_ids': tensor([[ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([100, 4])\n",
      "tensor([[0.2174, 0.6936, 0.0794, 0.0096],\n",
      "        [0.2566, 0.6762, 0.0624, 0.0049],\n",
      "        [0.2185, 0.6916, 0.0803, 0.0095],\n",
      "        [0.2202, 0.6920, 0.0790, 0.0088],\n",
      "        [0.2234, 0.6924, 0.0762, 0.0079],\n",
      "        [0.2195, 0.6917, 0.0797, 0.0091],\n",
      "        [0.2190, 0.6915, 0.0802, 0.0093],\n",
      "        [0.2205, 0.6921, 0.0787, 0.0087],\n",
      "        [0.2197, 0.6918, 0.0795, 0.0090],\n",
      "        [0.2180, 0.6923, 0.0801, 0.0096],\n",
      "        [0.2190, 0.6915, 0.0801, 0.0093],\n",
      "        [0.2067, 0.7163, 0.0689, 0.0081],\n",
      "        [0.2185, 0.6916, 0.0803, 0.0095],\n",
      "        [0.2534, 0.6782, 0.0633, 0.0050],\n",
      "        [0.2201, 0.6920, 0.0791, 0.0088],\n",
      "        [0.2338, 0.6892, 0.0705, 0.0065],\n",
      "        [0.2192, 0.6916, 0.0800, 0.0092],\n",
      "        [0.2180, 0.6923, 0.0800, 0.0096],\n",
      "        [0.2286, 0.6913, 0.0730, 0.0071],\n",
      "        [0.2190, 0.6915, 0.0801, 0.0093],\n",
      "        [0.2181, 0.6922, 0.0801, 0.0096],\n",
      "        [0.2207, 0.6922, 0.0785, 0.0086],\n",
      "        [0.2172, 0.6940, 0.0792, 0.0096],\n",
      "        [0.2204, 0.6921, 0.0788, 0.0087],\n",
      "        [0.2281, 0.6914, 0.0733, 0.0072],\n",
      "        [0.2232, 0.6924, 0.0764, 0.0080],\n",
      "        [0.2271, 0.6917, 0.0739, 0.0073],\n",
      "        [0.2221, 0.6924, 0.0772, 0.0082],\n",
      "        [0.2243, 0.6923, 0.0756, 0.0078],\n",
      "        [0.2216, 0.6924, 0.0776, 0.0084],\n",
      "        [0.2198, 0.6918, 0.0794, 0.0090],\n",
      "        [0.2172, 0.6940, 0.0792, 0.0096],\n",
      "        [0.2171, 0.6943, 0.0790, 0.0096],\n",
      "        [0.2190, 0.6915, 0.0801, 0.0093],\n",
      "        [0.2143, 0.7005, 0.0760, 0.0092],\n",
      "        [0.2210, 0.6923, 0.0782, 0.0085],\n",
      "        [0.2182, 0.6919, 0.0802, 0.0096],\n",
      "        [0.2361, 0.6881, 0.0695, 0.0063],\n",
      "        [0.2149, 0.6992, 0.0766, 0.0093],\n",
      "        [0.2067, 0.7163, 0.0689, 0.0081],\n",
      "        [0.2212, 0.6923, 0.0780, 0.0085],\n",
      "        [0.2281, 0.6914, 0.0733, 0.0072],\n",
      "        [0.2080, 0.7137, 0.0700, 0.0083],\n",
      "        [0.2196, 0.6918, 0.0796, 0.0090],\n",
      "        [0.2183, 0.6919, 0.0803, 0.0096],\n",
      "        [0.2202, 0.6920, 0.0789, 0.0088],\n",
      "        [0.2196, 0.6918, 0.0796, 0.0091],\n",
      "        [0.2116, 0.7063, 0.0733, 0.0088],\n",
      "        [0.2209, 0.6922, 0.0783, 0.0086],\n",
      "        [0.2248, 0.6922, 0.0753, 0.0077],\n",
      "        [0.2177, 0.6929, 0.0798, 0.0096],\n",
      "        [0.2254, 0.6921, 0.0749, 0.0076],\n",
      "        [0.2067, 0.7163, 0.0689, 0.0081],\n",
      "        [0.2207, 0.6922, 0.0785, 0.0086],\n",
      "        [0.2067, 0.7163, 0.0689, 0.0081],\n",
      "        [0.2201, 0.6920, 0.0791, 0.0089],\n",
      "        [0.2185, 0.6916, 0.0803, 0.0095],\n",
      "        [0.2186, 0.6915, 0.0803, 0.0095],\n",
      "        [0.2179, 0.6925, 0.0800, 0.0096],\n",
      "        [0.2205, 0.6921, 0.0786, 0.0087],\n",
      "        [0.2389, 0.6867, 0.0683, 0.0060],\n",
      "        [0.2113, 0.7070, 0.0729, 0.0087],\n",
      "        [0.2186, 0.6915, 0.0803, 0.0095],\n",
      "        [0.2189, 0.6915, 0.0803, 0.0094],\n",
      "        [0.2205, 0.6921, 0.0786, 0.0087],\n",
      "        [0.2067, 0.7163, 0.0689, 0.0081],\n",
      "        [0.2211, 0.6923, 0.0781, 0.0085],\n",
      "        [0.2151, 0.6988, 0.0768, 0.0093],\n",
      "        [0.2183, 0.6918, 0.0803, 0.0096],\n",
      "        [0.2179, 0.6926, 0.0799, 0.0096],\n",
      "        [0.2188, 0.6915, 0.0803, 0.0094],\n",
      "        [0.2218, 0.6924, 0.0775, 0.0083],\n",
      "        [0.2338, 0.6892, 0.0705, 0.0065],\n",
      "        [0.2239, 0.6924, 0.0759, 0.0079],\n",
      "        [0.2229, 0.6924, 0.0766, 0.0080],\n",
      "        [0.2238, 0.6924, 0.0760, 0.0079],\n",
      "        [0.2179, 0.6926, 0.0799, 0.0096],\n",
      "        [0.2173, 0.6938, 0.0793, 0.0096],\n",
      "        [0.2151, 0.6988, 0.0768, 0.0093],\n",
      "        [0.2243, 0.6923, 0.0756, 0.0078],\n",
      "        [0.2169, 0.6946, 0.0789, 0.0096],\n",
      "        [0.2287, 0.6912, 0.0730, 0.0071],\n",
      "        [0.2173, 0.6939, 0.0793, 0.0096],\n",
      "        [0.2580, 0.6753, 0.0620, 0.0048],\n",
      "        [0.2190, 0.6915, 0.0801, 0.0093],\n",
      "        [0.2166, 0.6953, 0.0785, 0.0095],\n",
      "        [0.2145, 0.7002, 0.0761, 0.0092],\n",
      "        [0.2240, 0.6924, 0.0758, 0.0078],\n",
      "        [0.2074, 0.7149, 0.0695, 0.0082],\n",
      "        [0.2271, 0.6917, 0.0739, 0.0073],\n",
      "        [0.2166, 0.6953, 0.0785, 0.0095],\n",
      "        [0.2334, 0.6894, 0.0707, 0.0065],\n",
      "        [0.2149, 0.6993, 0.0766, 0.0093],\n",
      "        [0.2067, 0.7163, 0.0689, 0.0081],\n",
      "        [0.2210, 0.6923, 0.0782, 0.0085],\n",
      "        [0.2184, 0.6918, 0.0803, 0.0096],\n",
      "        [0.2173, 0.6937, 0.0793, 0.0096],\n",
      "        [0.2155, 0.6978, 0.0773, 0.0094],\n",
      "        [0.2536, 0.6781, 0.0633, 0.0050],\n",
      "        [0.2146, 0.6999, 0.0762, 0.0093]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "{'input_ids': tensor([[ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_data, batch_label = next(iter(train_loader))\n",
    "for k, v in batch_data.items():\n",
    "    batch_data[k] = v.to(device)\n",
    "    print(v.shape)\n",
    "# batch_label = batch_label.to(device)\n",
    "for k, v in batch_label.items():\n",
    "    batch_label[k] = v.to(device)\n",
    "\n",
    "print(batch_label)\n",
    "\n",
    "batch_logits = model_mit(batch_data)\n",
    "print(batch_logits.shape)\n",
    "print(batch_logits)\n",
    "print(batch_label)\n",
    "print(batch_data['input_ids'].shape)\n",
    "# print(batch_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "uXr_jMj-aJID",
    "outputId": "56ded215-ae28-42b3-e20e-9ef8a44493d7"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for data, data2 in tqdm(train_loader):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "\n",
    "        for k, v in data2.items():\n",
    "            data2[k] = v.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output2 = model(data2)\n",
    "        loss = criterion(output, output2)\n",
    "        loss = Variable(loss, requires_grad = True)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = output.argmax(dim=1)\n",
    "        lab = output2.argmax(dim=1)\n",
    "        correct += pred.eq(lab).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    acc = correct/len(train_loader.dataset)\n",
    "\n",
    "    return train_loss, acc\n",
    "\n",
    "\n",
    "def val(model, val_loader, criterion, min_val_loss):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0.\n",
    "    with torch.no_grad():\n",
    "        for data, data2 in val_loader:\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "\n",
    "            for k, v in data2.items():\n",
    "                data2[k] = v.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            output2 = model(data2)\n",
    "            val_loss += criterion(output, output2).item()\n",
    "\n",
    "            pred = output.argmax(dim=1)\n",
    "            lab = output2.argmax(dim=1)\n",
    "            correct += pred.eq(lab).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        acc = correct/len(val_loader.dataset)\n",
    "\n",
    "    if min_val_loss>val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best-model_mit-parameters.pt')\n",
    "        print('model saved')\n",
    "\n",
    "    return val_loss, acc, min_val_loss\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, optimizer, criterion):\n",
    "    loss_train_per_epoch = []\n",
    "    acc_train_per_epoch = []\n",
    "    loss_val_per_epoch = []\n",
    "    acc_val_per_epoch = []\n",
    "    min_val_loss = 1000\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, epoch+1)\n",
    "        val_loss, val_acc, min_val_loss = val(model, val_loader, criterion, min_val_loss)\n",
    "\n",
    "        print(f'[{epoch + 1}, {len(train_loader) + 1:5d}] loss: {train_loss:.3f}, accuracy: {train_acc:.3f} loss_val: {val_loss:.3f}, accuracy_val: {val_acc:.3f}')\n",
    "\n",
    "        loss_train_per_epoch += [train_loss]\n",
    "        acc_train_per_epoch += [train_acc]\n",
    "        loss_val_per_epoch += [val_loss]\n",
    "        acc_val_per_epoch += [val_acc]\n",
    "\n",
    "    return loss_train_per_epoch, loss_val_per_epoch, acc_train_per_epoch, acc_val_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "dz64JCoHB9pi",
    "outputId": "fe9148d7-bd70-4c75-92e6-e8fa0cf0679d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 26/26 [01:44<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "[1,    27] loss: 0.000, accuracy: 1.000 loss_val: 0.000, accuracy_val: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▍                                        | 2/26 [00:12<02:30,  6.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# criterion = nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 5\u001b[0m loss_train_per_epoch, loss_val_per_epoch, acc_train_per_epoch, acc_val_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 66\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, train_loader, val_loader, epochs, optimizer, criterion)\u001b[0m\n\u001b[1;32m     64\u001b[0m min_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 66\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     val_loss, val_acc, min_val_loss \u001b[38;5;241m=\u001b[39m val(model, val_loader, criterion, min_val_loss)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss_val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy_val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[51], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, output2)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m Variable(loss, requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "epochs=10\n",
    "loss_train_per_epoch, loss_val_per_epoch, acc_train_per_epoch, acc_val_per_epoch = fit(model_mit, train_loader, val_loader, epochs, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mit.classifier[-1] = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "{'input_ids': tensor([[ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([1404])\n",
      "tensor([0.0002, 0.0008, 0.0003,  ..., 0.0011, 0.0004, 0.0002], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "{'input_ids': tensor([[ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_data, batch_label = next(iter(train_loader))\n",
    "for k, v in batch_data.items():\n",
    "    batch_data[k] = v.to(device)\n",
    "    print(v.shape)\n",
    "# batch_label = batch_label.to(device)\n",
    "for k, v in batch_label.items():\n",
    "    batch_label[k] = v.to(device)\n",
    "\n",
    "print(batch_label)\n",
    "\n",
    "batch_logits = model_mit(batch_data)\n",
    "print(batch_logits.shape)\n",
    "print(batch_logits)\n",
    "print(batch_label)\n",
    "print(batch_data['input_ids'].shape)\n",
    "# print(batch_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Bert_mitigator:\n\tMissing key(s) in state_dict: \"LMRec.bert.embeddings.word_embeddings.weight\", \"LMRec.bert.embeddings.position_embeddings.weight\", \"LMRec.bert.embeddings.token_type_embeddings.weight\", \"LMRec.bert.embeddings.LayerNorm.weight\", \"LMRec.bert.embeddings.LayerNorm.bias\", \"LMRec.bert.encoder.layer.0.attention.self.query.weight\", \"LMRec.bert.encoder.layer.0.attention.self.query.bias\", \"LMRec.bert.encoder.layer.0.attention.self.key.weight\", \"LMRec.bert.encoder.layer.0.attention.self.key.bias\", \"LMRec.bert.encoder.layer.0.attention.self.value.weight\", \"LMRec.bert.encoder.layer.0.attention.self.value.bias\", \"LMRec.bert.encoder.layer.0.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.0.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.0.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.0.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.0.output.dense.weight\", \"LMRec.bert.encoder.layer.0.output.dense.bias\", \"LMRec.bert.encoder.layer.0.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.0.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.1.attention.self.query.weight\", \"LMRec.bert.encoder.layer.1.attention.self.query.bias\", \"LMRec.bert.encoder.layer.1.attention.self.key.weight\", \"LMRec.bert.encoder.layer.1.attention.self.key.bias\", \"LMRec.bert.encoder.layer.1.attention.self.value.weight\", \"LMRec.bert.encoder.layer.1.attention.self.value.bias\", \"LMRec.bert.encoder.layer.1.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.1.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.1.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.1.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.1.output.dense.weight\", \"LMRec.bert.encoder.layer.1.output.dense.bias\", \"LMRec.bert.encoder.layer.1.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.1.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.2.attention.self.query.weight\", \"LMRec.bert.encoder.layer.2.attention.self.query.bias\", \"LMRec.bert.encoder.layer.2.attention.self.key.weight\", \"LMRec.bert.encoder.layer.2.attention.self.key.bias\", \"LMRec.bert.encoder.layer.2.attention.self.value.weight\", \"LMRec.bert.encoder.layer.2.attention.self.value.bias\", \"LMRec.bert.encoder.layer.2.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.2.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.2.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.2.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.2.output.dense.weight\", \"LMRec.bert.encoder.layer.2.output.dense.bias\", \"LMRec.bert.encoder.layer.2.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.2.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.3.attention.self.query.weight\", \"LMRec.bert.encoder.layer.3.attention.self.query.bias\", \"LMRec.bert.encoder.layer.3.attention.self.key.weight\", \"LMRec.bert.encoder.layer.3.attention.self.key.bias\", \"LMRec.bert.encoder.layer.3.attention.self.value.weight\", \"LMRec.bert.encoder.layer.3.attention.self.value.bias\", \"LMRec.bert.encoder.layer.3.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.3.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.3.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.3.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.3.output.dense.weight\", \"LMRec.bert.encoder.layer.3.output.dense.bias\", \"LMRec.bert.encoder.layer.3.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.3.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.4.attention.self.query.weight\", \"LMRec.bert.encoder.layer.4.attention.self.query.bias\", \"LMRec.bert.encoder.layer.4.attention.self.key.weight\", \"LMRec.bert.encoder.layer.4.attention.self.key.bias\", \"LMRec.bert.encoder.layer.4.attention.self.value.weight\", \"LMRec.bert.encoder.layer.4.attention.self.value.bias\", \"LMRec.bert.encoder.layer.4.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.4.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.4.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.4.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.4.output.dense.weight\", \"LMRec.bert.encoder.layer.4.output.dense.bias\", \"LMRec.bert.encoder.layer.4.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.4.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.5.attention.self.query.weight\", \"LMRec.bert.encoder.layer.5.attention.self.query.bias\", \"LMRec.bert.encoder.layer.5.attention.self.key.weight\", \"LMRec.bert.encoder.layer.5.attention.self.key.bias\", \"LMRec.bert.encoder.layer.5.attention.self.value.weight\", \"LMRec.bert.encoder.layer.5.attention.self.value.bias\", \"LMRec.bert.encoder.layer.5.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.5.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.5.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.5.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.5.output.dense.weight\", \"LMRec.bert.encoder.layer.5.output.dense.bias\", \"LMRec.bert.encoder.layer.5.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.5.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.6.attention.self.query.weight\", \"LMRec.bert.encoder.layer.6.attention.self.query.bias\", \"LMRec.bert.encoder.layer.6.attention.self.key.weight\", \"LMRec.bert.encoder.layer.6.attention.self.key.bias\", \"LMRec.bert.encoder.layer.6.attention.self.value.weight\", \"LMRec.bert.encoder.layer.6.attention.self.value.bias\", \"LMRec.bert.encoder.layer.6.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.6.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.6.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.6.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.6.output.dense.weight\", \"LMRec.bert.encoder.layer.6.output.dense.bias\", \"LMRec.bert.encoder.layer.6.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.6.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.7.attention.self.query.weight\", \"LMRec.bert.encoder.layer.7.attention.self.query.bias\", \"LMRec.bert.encoder.layer.7.attention.self.key.weight\", \"LMRec.bert.encoder.layer.7.attention.self.key.bias\", \"LMRec.bert.encoder.layer.7.attention.self.value.weight\", \"LMRec.bert.encoder.layer.7.attention.self.value.bias\", \"LMRec.bert.encoder.layer.7.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.7.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.7.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.7.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.7.output.dense.weight\", \"LMRec.bert.encoder.layer.7.output.dense.bias\", \"LMRec.bert.encoder.layer.7.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.7.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.8.attention.self.query.weight\", \"LMRec.bert.encoder.layer.8.attention.self.query.bias\", \"LMRec.bert.encoder.layer.8.attention.self.key.weight\", \"LMRec.bert.encoder.layer.8.attention.self.key.bias\", \"LMRec.bert.encoder.layer.8.attention.self.value.weight\", \"LMRec.bert.encoder.layer.8.attention.self.value.bias\", \"LMRec.bert.encoder.layer.8.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.8.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.8.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.8.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.8.output.dense.weight\", \"LMRec.bert.encoder.layer.8.output.dense.bias\", \"LMRec.bert.encoder.layer.8.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.8.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.9.attention.self.query.weight\", \"LMRec.bert.encoder.layer.9.attention.self.query.bias\", \"LMRec.bert.encoder.layer.9.attention.self.key.weight\", \"LMRec.bert.encoder.layer.9.attention.self.key.bias\", \"LMRec.bert.encoder.layer.9.attention.self.value.weight\", \"LMRec.bert.encoder.layer.9.attention.self.value.bias\", \"LMRec.bert.encoder.layer.9.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.9.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.9.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.9.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.9.output.dense.weight\", \"LMRec.bert.encoder.layer.9.output.dense.bias\", \"LMRec.bert.encoder.layer.9.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.9.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.10.attention.self.query.weight\", \"LMRec.bert.encoder.layer.10.attention.self.query.bias\", \"LMRec.bert.encoder.layer.10.attention.self.key.weight\", \"LMRec.bert.encoder.layer.10.attention.self.key.bias\", \"LMRec.bert.encoder.layer.10.attention.self.value.weight\", \"LMRec.bert.encoder.layer.10.attention.self.value.bias\", \"LMRec.bert.encoder.layer.10.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.10.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.10.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.10.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.10.output.dense.weight\", \"LMRec.bert.encoder.layer.10.output.dense.bias\", \"LMRec.bert.encoder.layer.10.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.10.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.11.attention.self.query.weight\", \"LMRec.bert.encoder.layer.11.attention.self.query.bias\", \"LMRec.bert.encoder.layer.11.attention.self.key.weight\", \"LMRec.bert.encoder.layer.11.attention.self.key.bias\", \"LMRec.bert.encoder.layer.11.attention.self.value.weight\", \"LMRec.bert.encoder.layer.11.attention.self.value.bias\", \"LMRec.bert.encoder.layer.11.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.11.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.11.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.11.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.11.output.dense.weight\", \"LMRec.bert.encoder.layer.11.output.dense.bias\", \"LMRec.bert.encoder.layer.11.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.11.output.LayerNorm.bias\", \"LMRec.bert.pooler.dense.weight\", \"LMRec.bert.pooler.dense.bias\", \"LMRec.classifier.0.weight\", \"LMRec.classifier.0.bias\", \"LMRec.classifier.2.weight\", \"LMRec.classifier.2.bias\", \"mitigator.0.weight\", \"mitigator.0.bias\", \"mitigator.2.weight\", \"mitigator.2.bias\". \n\tUnexpected key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.2.weight\", \"classifier.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/Atlanta/model_mit.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Bert_mitigator:\n\tMissing key(s) in state_dict: \"LMRec.bert.embeddings.word_embeddings.weight\", \"LMRec.bert.embeddings.position_embeddings.weight\", \"LMRec.bert.embeddings.token_type_embeddings.weight\", \"LMRec.bert.embeddings.LayerNorm.weight\", \"LMRec.bert.embeddings.LayerNorm.bias\", \"LMRec.bert.encoder.layer.0.attention.self.query.weight\", \"LMRec.bert.encoder.layer.0.attention.self.query.bias\", \"LMRec.bert.encoder.layer.0.attention.self.key.weight\", \"LMRec.bert.encoder.layer.0.attention.self.key.bias\", \"LMRec.bert.encoder.layer.0.attention.self.value.weight\", \"LMRec.bert.encoder.layer.0.attention.self.value.bias\", \"LMRec.bert.encoder.layer.0.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.0.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.0.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.0.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.0.output.dense.weight\", \"LMRec.bert.encoder.layer.0.output.dense.bias\", \"LMRec.bert.encoder.layer.0.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.0.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.1.attention.self.query.weight\", \"LMRec.bert.encoder.layer.1.attention.self.query.bias\", \"LMRec.bert.encoder.layer.1.attention.self.key.weight\", \"LMRec.bert.encoder.layer.1.attention.self.key.bias\", \"LMRec.bert.encoder.layer.1.attention.self.value.weight\", \"LMRec.bert.encoder.layer.1.attention.self.value.bias\", \"LMRec.bert.encoder.layer.1.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.1.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.1.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.1.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.1.output.dense.weight\", \"LMRec.bert.encoder.layer.1.output.dense.bias\", \"LMRec.bert.encoder.layer.1.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.1.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.2.attention.self.query.weight\", \"LMRec.bert.encoder.layer.2.attention.self.query.bias\", \"LMRec.bert.encoder.layer.2.attention.self.key.weight\", \"LMRec.bert.encoder.layer.2.attention.self.key.bias\", \"LMRec.bert.encoder.layer.2.attention.self.value.weight\", \"LMRec.bert.encoder.layer.2.attention.self.value.bias\", \"LMRec.bert.encoder.layer.2.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.2.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.2.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.2.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.2.output.dense.weight\", \"LMRec.bert.encoder.layer.2.output.dense.bias\", \"LMRec.bert.encoder.layer.2.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.2.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.3.attention.self.query.weight\", \"LMRec.bert.encoder.layer.3.attention.self.query.bias\", \"LMRec.bert.encoder.layer.3.attention.self.key.weight\", \"LMRec.bert.encoder.layer.3.attention.self.key.bias\", \"LMRec.bert.encoder.layer.3.attention.self.value.weight\", \"LMRec.bert.encoder.layer.3.attention.self.value.bias\", \"LMRec.bert.encoder.layer.3.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.3.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.3.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.3.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.3.output.dense.weight\", \"LMRec.bert.encoder.layer.3.output.dense.bias\", \"LMRec.bert.encoder.layer.3.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.3.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.4.attention.self.query.weight\", \"LMRec.bert.encoder.layer.4.attention.self.query.bias\", \"LMRec.bert.encoder.layer.4.attention.self.key.weight\", \"LMRec.bert.encoder.layer.4.attention.self.key.bias\", \"LMRec.bert.encoder.layer.4.attention.self.value.weight\", \"LMRec.bert.encoder.layer.4.attention.self.value.bias\", \"LMRec.bert.encoder.layer.4.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.4.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.4.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.4.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.4.output.dense.weight\", \"LMRec.bert.encoder.layer.4.output.dense.bias\", \"LMRec.bert.encoder.layer.4.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.4.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.5.attention.self.query.weight\", \"LMRec.bert.encoder.layer.5.attention.self.query.bias\", \"LMRec.bert.encoder.layer.5.attention.self.key.weight\", \"LMRec.bert.encoder.layer.5.attention.self.key.bias\", \"LMRec.bert.encoder.layer.5.attention.self.value.weight\", \"LMRec.bert.encoder.layer.5.attention.self.value.bias\", \"LMRec.bert.encoder.layer.5.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.5.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.5.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.5.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.5.output.dense.weight\", \"LMRec.bert.encoder.layer.5.output.dense.bias\", \"LMRec.bert.encoder.layer.5.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.5.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.6.attention.self.query.weight\", \"LMRec.bert.encoder.layer.6.attention.self.query.bias\", \"LMRec.bert.encoder.layer.6.attention.self.key.weight\", \"LMRec.bert.encoder.layer.6.attention.self.key.bias\", \"LMRec.bert.encoder.layer.6.attention.self.value.weight\", \"LMRec.bert.encoder.layer.6.attention.self.value.bias\", \"LMRec.bert.encoder.layer.6.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.6.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.6.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.6.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.6.output.dense.weight\", \"LMRec.bert.encoder.layer.6.output.dense.bias\", \"LMRec.bert.encoder.layer.6.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.6.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.7.attention.self.query.weight\", \"LMRec.bert.encoder.layer.7.attention.self.query.bias\", \"LMRec.bert.encoder.layer.7.attention.self.key.weight\", \"LMRec.bert.encoder.layer.7.attention.self.key.bias\", \"LMRec.bert.encoder.layer.7.attention.self.value.weight\", \"LMRec.bert.encoder.layer.7.attention.self.value.bias\", \"LMRec.bert.encoder.layer.7.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.7.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.7.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.7.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.7.output.dense.weight\", \"LMRec.bert.encoder.layer.7.output.dense.bias\", \"LMRec.bert.encoder.layer.7.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.7.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.8.attention.self.query.weight\", \"LMRec.bert.encoder.layer.8.attention.self.query.bias\", \"LMRec.bert.encoder.layer.8.attention.self.key.weight\", \"LMRec.bert.encoder.layer.8.attention.self.key.bias\", \"LMRec.bert.encoder.layer.8.attention.self.value.weight\", \"LMRec.bert.encoder.layer.8.attention.self.value.bias\", \"LMRec.bert.encoder.layer.8.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.8.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.8.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.8.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.8.output.dense.weight\", \"LMRec.bert.encoder.layer.8.output.dense.bias\", \"LMRec.bert.encoder.layer.8.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.8.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.9.attention.self.query.weight\", \"LMRec.bert.encoder.layer.9.attention.self.query.bias\", \"LMRec.bert.encoder.layer.9.attention.self.key.weight\", \"LMRec.bert.encoder.layer.9.attention.self.key.bias\", \"LMRec.bert.encoder.layer.9.attention.self.value.weight\", \"LMRec.bert.encoder.layer.9.attention.self.value.bias\", \"LMRec.bert.encoder.layer.9.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.9.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.9.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.9.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.9.output.dense.weight\", \"LMRec.bert.encoder.layer.9.output.dense.bias\", \"LMRec.bert.encoder.layer.9.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.9.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.10.attention.self.query.weight\", \"LMRec.bert.encoder.layer.10.attention.self.query.bias\", \"LMRec.bert.encoder.layer.10.attention.self.key.weight\", \"LMRec.bert.encoder.layer.10.attention.self.key.bias\", \"LMRec.bert.encoder.layer.10.attention.self.value.weight\", \"LMRec.bert.encoder.layer.10.attention.self.value.bias\", \"LMRec.bert.encoder.layer.10.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.10.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.10.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.10.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.10.output.dense.weight\", \"LMRec.bert.encoder.layer.10.output.dense.bias\", \"LMRec.bert.encoder.layer.10.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.10.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.11.attention.self.query.weight\", \"LMRec.bert.encoder.layer.11.attention.self.query.bias\", \"LMRec.bert.encoder.layer.11.attention.self.key.weight\", \"LMRec.bert.encoder.layer.11.attention.self.key.bias\", \"LMRec.bert.encoder.layer.11.attention.self.value.weight\", \"LMRec.bert.encoder.layer.11.attention.self.value.bias\", \"LMRec.bert.encoder.layer.11.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.11.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.11.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.11.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.11.output.dense.weight\", \"LMRec.bert.encoder.layer.11.output.dense.bias\", \"LMRec.bert.encoder.layer.11.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.11.output.LayerNorm.bias\", \"LMRec.bert.pooler.dense.weight\", \"LMRec.bert.pooler.dense.bias\", \"LMRec.classifier.0.weight\", \"LMRec.classifier.0.bias\", \"LMRec.classifier.2.weight\", \"LMRec.classifier.2.bias\", \"mitigator.0.weight\", \"mitigator.0.bias\", \"mitigator.2.weight\", \"mitigator.2.bias\". \n\tUnexpected key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.2.weight\", \"classifier.2.bias\". "
     ]
    }
   ],
   "source": [
    "model_mit.load_state_dict(torch.load('models/'+CITY+'/model_mit.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMRec.bert.embeddings.word_embeddings.weight\n",
      "LMRec.bert.embeddings.position_embeddings.weight\n",
      "LMRec.bert.embeddings.token_type_embeddings.weight\n",
      "LMRec.bert.embeddings.LayerNorm.weight\n",
      "LMRec.bert.embeddings.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.0.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.0.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.0.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.0.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.0.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.0.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.0.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.0.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.0.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.0.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.0.output.dense.weight\n",
      "LMRec.bert.encoder.layer.0.output.dense.bias\n",
      "LMRec.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.1.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.1.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.1.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.1.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.1.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.1.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.1.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.1.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.1.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.1.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.1.output.dense.weight\n",
      "LMRec.bert.encoder.layer.1.output.dense.bias\n",
      "LMRec.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.2.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.2.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.2.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.2.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.2.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.2.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.2.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.2.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.2.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.2.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.2.output.dense.weight\n",
      "LMRec.bert.encoder.layer.2.output.dense.bias\n",
      "LMRec.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.3.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.3.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.3.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.3.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.3.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.3.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.3.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.3.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.3.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.3.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.3.output.dense.weight\n",
      "LMRec.bert.encoder.layer.3.output.dense.bias\n",
      "LMRec.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.4.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.4.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.4.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.4.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.4.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.4.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.4.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.4.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.4.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.4.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.4.output.dense.weight\n",
      "LMRec.bert.encoder.layer.4.output.dense.bias\n",
      "LMRec.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.5.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.5.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.5.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.5.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.5.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.5.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.5.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.5.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.5.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.5.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.5.output.dense.weight\n",
      "LMRec.bert.encoder.layer.5.output.dense.bias\n",
      "LMRec.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.6.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.6.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.6.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.6.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.6.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.6.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.6.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.6.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.6.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.6.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.6.output.dense.weight\n",
      "LMRec.bert.encoder.layer.6.output.dense.bias\n",
      "LMRec.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.7.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.7.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.7.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.7.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.7.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.7.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.7.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.7.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.7.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.7.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.7.output.dense.weight\n",
      "LMRec.bert.encoder.layer.7.output.dense.bias\n",
      "LMRec.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.8.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.8.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.8.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.8.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.8.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.8.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.8.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.8.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.8.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.8.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.8.output.dense.weight\n",
      "LMRec.bert.encoder.layer.8.output.dense.bias\n",
      "LMRec.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.9.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.9.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.9.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.9.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.9.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.9.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.9.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.9.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.9.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.9.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.9.output.dense.weight\n",
      "LMRec.bert.encoder.layer.9.output.dense.bias\n",
      "LMRec.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.10.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.10.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.10.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.10.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.10.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.10.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.10.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.10.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.10.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.10.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.10.output.dense.weight\n",
      "LMRec.bert.encoder.layer.10.output.dense.bias\n",
      "LMRec.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.11.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.11.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.11.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.11.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.11.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.11.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.11.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.11.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.11.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.11.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.11.output.dense.weight\n",
      "LMRec.bert.encoder.layer.11.output.dense.bias\n",
      "LMRec.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "LMRec.bert.pooler.dense.weight\n",
      "LMRec.bert.pooler.dense.bias\n",
      "LMRec.classifier.0.weight\n",
      "LMRec.classifier.0.bias\n",
      "LMRec.classifier.2.weight\n",
      "LMRec.classifier.2.bias\n",
      "mitigator.0.weight\n",
      "mitigator.0.bias\n",
      "mitigator.2.weight\n",
      "mitigator.2.bias\n"
     ]
    }
   ],
   "source": [
    "state_dict = model_mit.state_dict()\n",
    "\n",
    "# Get parameter names\n",
    "parameter_names = state_dict.keys()\n",
    "\n",
    "# Print parameter names\n",
    "for name in parameter_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
