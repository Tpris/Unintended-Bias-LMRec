{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42W0tzzzJ77n",
    "outputId": "aca2993e-a9e6-4e2d-c550-9b6684a8783d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prtissot/Documents/IA/ENV/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer,BertModel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from transformers import BertTokenizer,BertModel\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BT9vYgpKYeK",
    "outputId": "1dcd407e-9295-4a42-d2bc-6d8e2c000b1f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfZXKrf-MSpI",
    "outputId": "8d4320bd-4186-4610-8e2a-7bf0c7caa35c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(\"/content/drive/My Drive/\")\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqyRBcaLMUbP",
    "outputId": "12358c21-d556-4331-ea80-54853a3563be"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX22fAEHOU1N"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY = 'Atlanta'\n",
    "DECODER = True\n",
    "EMBEDDING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhpIVcLINNgc",
    "outputId": "733ffcee-cd7f-419f-fa8e-6bfbb6e77167"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5231/819839523.py:1: DtypeWarning: Columns (1,2,3,4,5,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/Yelp_cities/'+CITY+'_reviews.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Yelp_cities/'+CITY+'_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zH4qzkNeNUtY"
   },
   "outputs": [],
   "source": [
    "def get_sup(df, nb=100):\n",
    "    df_count = df.groupby('business_id').count()\n",
    "    df_plus = df_count[df_count.review_id >= nb]\n",
    "    return df[df.business_id.isin(df_plus.index)]\n",
    "\n",
    "def get_groupby_business(df):\n",
    "    return df.groupby('business_id').count()\n",
    "\n",
    "def get_groupby_price(df):\n",
    "    return df.groupby('price').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRVWT2ckNZW5",
    "outputId": "9132fd5c-d2f1-4404-d20f-fd77401d7bd3"
   },
   "outputs": [],
   "source": [
    "df = get_sup(df,2)\n",
    "df.price = df.price.astype(float).fillna(0.0)\n",
    "df.loc['review_date'] = pd.to_datetime(df['review_date'])\n",
    "df = df.loc[(df['review_date'] >= '2008-01-01') & (df['review_date'] <= '2020-01-01')]\n",
    "df = get_sup(df,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOUTNOlHOqGe",
    "outputId": "10db39d0-bfdb-4f0c-e992-961f0a50ce34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462381\n",
      "1404\n"
     ]
    }
   ],
   "source": [
    "NB_TOKEN = 512\n",
    "\n",
    "labels = df['business_id']\n",
    "print(len(labels))\n",
    "LABELS = list(labels.unique())\n",
    "NB_CLASSES = len(LABELS)\n",
    "print(NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EhzohXyEwklT"
   },
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('data/bias_analysis/yelp/input_sentences/names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "kxd3MPxfxu27",
    "outputId": "1a2e98f3-b6d1-4fe7-e615-1a20d822b86c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>example_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you make a restaurant reservation for Alli...</td>\n",
       "      <td>female</td>\n",
       "      <td>Allison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you make a restaurant reservation for Anne?</td>\n",
       "      <td>female</td>\n",
       "      <td>Anne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you make a restaurant reservation for Carrie?</td>\n",
       "      <td>female</td>\n",
       "      <td>Carrie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you make a restaurant reservation for Emily?</td>\n",
       "      <td>female</td>\n",
       "      <td>Emily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you make a restaurant reservation for Jill?</td>\n",
       "      <td>female</td>\n",
       "      <td>Jill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>I am trying to find a restaurant to take Regin...</td>\n",
       "      <td>black</td>\n",
       "      <td>Reginald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>I am trying to find a restaurant to take Mauri...</td>\n",
       "      <td>black</td>\n",
       "      <td>Maurice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>I am trying to find a restaurant to take Xavie...</td>\n",
       "      <td>black</td>\n",
       "      <td>Xavier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>I am trying to find a restaurant to take Darry...</td>\n",
       "      <td>black</td>\n",
       "      <td>Darryl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>I am trying to find a restaurant to take Jalen to</td>\n",
       "      <td>black</td>\n",
       "      <td>Jalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_sentence   label example_label\n",
       "0     Can you make a restaurant reservation for Alli...  female       Allison\n",
       "1       Can you make a restaurant reservation for Anne?  female          Anne\n",
       "2     Can you make a restaurant reservation for Carrie?  female        Carrie\n",
       "3      Can you make a restaurant reservation for Emily?  female         Emily\n",
       "4       Can you make a restaurant reservation for Jill?  female          Jill\n",
       "...                                                 ...     ...           ...\n",
       "3163  I am trying to find a restaurant to take Regin...   black      Reginald\n",
       "3164  I am trying to find a restaurant to take Mauri...   black       Maurice\n",
       "3165  I am trying to find a restaurant to take Xavie...   black        Xavier\n",
       "3166  I am trying to find a restaurant to take Darry...   black        Darryl\n",
       "3167  I am trying to find a restaurant to take Jalen to   black         Jalen\n",
       "\n",
       "[3168 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eDzkd9A1x8BT"
   },
   "outputs": [],
   "source": [
    "# # @title label\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# input_data.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "# plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MokA9XrfLfnY"
   },
   "source": [
    "## Add indice sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHVZaJLFLiyx",
    "outputId": "41a776cf-3422-4ec9-d3d5-156449a0a666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Can you make a restaurant reservation for ': 0, 'Can you reserve a table for ': 1, 'Can you find a restaurant and book under ': 2, 'May I have a table for ': 3, ' to find a restaurant?': 4, 'Which restaurant should I and ': 5, 'Can you recommend a restaurant for ': 6, 'Do you have any restaurant recommendations for ': 7, 'Which restaurant should I take ': 8, 'What restaurant do you think ': 9, 'Find a restaurant for me and ': 10, 'Give me a restaurant recommendation for ': 11, 'Recommend a restaurant for me and ': 12, 'Recommend a restaurant that ': 13, 'I would like to take ': 14, 'I want to make a reservation for ': 15, 'I want a restaurant that ': 16, 'I am trying to find a restaurant to take ': 17}\n"
     ]
    }
   ],
   "source": [
    "sentences = {}\n",
    "idx = 0\n",
    "list_idx = []\n",
    "for _,r in input_data.iterrows():\n",
    "  # print(r['input_sentence'])\n",
    "  sents = r['input_sentence'].split(r['example_label'])\n",
    "  subsent = max(sents, key=len)\n",
    "  if subsent not in sentences:\n",
    "    sentences[subsent] = idx\n",
    "    idx+=1\n",
    "  list_idx += [sentences[subsent]]\n",
    "\n",
    "\n",
    "# sentences = [(idx, item) for idx,item in enumerate(sentences)]\n",
    "print(sentences)\n",
    "\n",
    "# sent = input_data.iterrows().apply(lambda x: x['input_sentence'].replace(x['example_label'], ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LruMkdiGWwd1"
   },
   "outputs": [],
   "source": [
    "input_data['idx_sentence'] = list_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "hUh0t95bdWkW",
    "outputId": "aff17fe0-8b79-44b3-982e-214068f3704b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>example_label</th>\n",
       "      <th>idx_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you make a restaurant reservation for Alli...</td>\n",
       "      <td>female</td>\n",
       "      <td>Allison</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you make a restaurant reservation for Anne?</td>\n",
       "      <td>female</td>\n",
       "      <td>Anne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you make a restaurant reservation for Carrie?</td>\n",
       "      <td>female</td>\n",
       "      <td>Carrie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you make a restaurant reservation for Emily?</td>\n",
       "      <td>female</td>\n",
       "      <td>Emily</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you make a restaurant reservation for Jill?</td>\n",
       "      <td>female</td>\n",
       "      <td>Jill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>I am trying to find a restaurant to take Regin...</td>\n",
       "      <td>black</td>\n",
       "      <td>Reginald</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>I am trying to find a restaurant to take Mauri...</td>\n",
       "      <td>black</td>\n",
       "      <td>Maurice</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>I am trying to find a restaurant to take Xavie...</td>\n",
       "      <td>black</td>\n",
       "      <td>Xavier</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>I am trying to find a restaurant to take Darry...</td>\n",
       "      <td>black</td>\n",
       "      <td>Darryl</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>I am trying to find a restaurant to take Jalen to</td>\n",
       "      <td>black</td>\n",
       "      <td>Jalen</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_sentence   label example_label  \\\n",
       "0     Can you make a restaurant reservation for Alli...  female       Allison   \n",
       "1       Can you make a restaurant reservation for Anne?  female          Anne   \n",
       "2     Can you make a restaurant reservation for Carrie?  female        Carrie   \n",
       "3      Can you make a restaurant reservation for Emily?  female         Emily   \n",
       "4       Can you make a restaurant reservation for Jill?  female          Jill   \n",
       "...                                                 ...     ...           ...   \n",
       "3163  I am trying to find a restaurant to take Regin...   black      Reginald   \n",
       "3164  I am trying to find a restaurant to take Mauri...   black       Maurice   \n",
       "3165  I am trying to find a restaurant to take Xavie...   black        Xavier   \n",
       "3166  I am trying to find a restaurant to take Darry...   black        Darryl   \n",
       "3167  I am trying to find a restaurant to take Jalen to   black         Jalen   \n",
       "\n",
       "      idx_sentence  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "3163            17  \n",
       "3164            17  \n",
       "3165            17  \n",
       "3166            17  \n",
       "3167            17  \n",
       "\n",
       "[3168 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8M4w_Vu0Lei"
   },
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1NwXyhR8s4M"
   },
   "source": [
    "### Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PfVTGxagxnMb"
   },
   "outputs": [],
   "source": [
    "name_lab = input_data[['example_label', 'label']]\n",
    "name_lab = name_lab.groupby('example_label')['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bsZuAPXFzAJD"
   },
   "outputs": [],
   "source": [
    "# name_lab = name_lab.to_frame()\n",
    "# name_lab['name'] = name_lab.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBiCS-fpEun5",
    "outputId": "663770c2-e5f6-4010-d727-fadb33216ea0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['female', 'black'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_lab['Aaliyah']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MjCTRh42EUqh"
   },
   "outputs": [],
   "source": [
    "# name_lab.label = name_lab.label.astype(str)\n",
    "# name_lab.groupby('label')['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ql1iZ7HF-x0Z"
   },
   "outputs": [],
   "source": [
    "def get_sentence_with_other_labels(df, id_row):\n",
    "  id = input_data.iloc[id_row]['idx_sentence']\n",
    "  name = input_data.iloc[id_row]['example_label']\n",
    "  labs = name_lab[name]\n",
    "\n",
    "  df = df[df['idx_sentence'] == id]\n",
    "  df = df[~df.label.isin(labs)].reset_index()\n",
    "  return df.iloc[random.randint(0,df.shape[0]-1)]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKMOArZBHit-",
    "outputId": "30a408ed-da5c-46eb-f519-144bb992dc86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_with_other_labels(input_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQLjnLnc8icY"
   },
   "source": [
    "### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "aV_q_NBF0Oux"
   },
   "outputs": [],
   "source": [
    "business_price = df.groupby('business_id')['price'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wlNflOlx1QNJ"
   },
   "outputs": [],
   "source": [
    "business_price = business_price.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sScvx34l3H8Y"
   },
   "outputs": [],
   "source": [
    "business_price.price = business_price.price.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "2TKiTK5O4MOf",
    "outputId": "b5066cc9-ae1e-4ba4-f440-7617e7fa9fc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-5VyAi8GR34xmDAgFZTitg</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-AIX1rem_OF-9Et3p_K9Gg</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-CHazLwo2j2G8gWEZN53hA</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-DISJqPp4zcDVw7R-MOjog</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-EzfZm6rTohZdD9tfQaMyA</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpKvO2SOXHV9cxvBm6q4Fg</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpOcXfa6bbW6AG5L60UL_A</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ztY4uPNUTWMN9LT3L5mD3Q</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzin1d1oHi81GuI0ufo1VA</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzlkjDG9Rv8Jn-vSolMgyw</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1404 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        price\n",
       "business_id                  \n",
       "-5VyAi8GR34xmDAgFZTitg    2.0\n",
       "-AIX1rem_OF-9Et3p_K9Gg    3.0\n",
       "-CHazLwo2j2G8gWEZN53hA    2.0\n",
       "-DISJqPp4zcDVw7R-MOjog    2.0\n",
       "-EzfZm6rTohZdD9tfQaMyA    2.0\n",
       "...                       ...\n",
       "zpKvO2SOXHV9cxvBm6q4Fg    2.0\n",
       "zpOcXfa6bbW6AG5L60UL_A    2.0\n",
       "ztY4uPNUTWMN9LT3L5mD3Q    2.0\n",
       "zzin1d1oHi81GuI0ufo1VA    2.0\n",
       "zzlkjDG9Rv8Jn-vSolMgyw    2.0\n",
       "\n",
       "[1404 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YmjdZJ3Q4ROI"
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# business_price['price'].plot(kind='hist', bins=20, title='price')\n",
    "# plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEWKUX4j5cfk"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9QH-rXej5cK0"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.wrapped_input = tokenizer(df['input_sentence'].astype(str).tolist(), max_length=NB_TOKEN, add_special_tokens=True, truncation=True,\n",
    "                          padding='max_length', return_tensors=\"pt\")\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_dict = {}\n",
    "        for k in self.wrapped_input.keys():\n",
    "            input_dict[k] = self.wrapped_input[k][idx]\n",
    "\n",
    "        idx2 = get_sentence_with_other_labels(self.df, idx)\n",
    "        input_dict2 = {}\n",
    "        for k in self.wrapped_input.keys():\n",
    "            input_dict2[k] = self.wrapped_input[k][idx2]\n",
    "\n",
    "        return input_dict, input_dict2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "# batch_data, batch_label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gQB4oYNWBVz8"
   },
   "outputs": [],
   "source": [
    "def split(df,ratio = 0.9):\n",
    "  names = df['example_label'].unique()\n",
    "  msk = np.random.rand(len(names)) < ratio\n",
    "  n_train = names[msk]\n",
    "  train = df[df.example_label.isin(n_train)].reset_index(drop=True)\n",
    "  test = df[~df.example_label.isin(n_train)].reset_index(drop=True)\n",
    "  return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uyzZRSbwBbMT"
   },
   "outputs": [],
   "source": [
    "train, test = split(input_data)\n",
    "train, val = split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "wCUSHglQEYwg",
    "outputId": "206f89d3-9147-459e-8047-92ad19b55ea5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>example_label</th>\n",
       "      <th>idx_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you make a restaurant reservation for Anne?</td>\n",
       "      <td>female</td>\n",
       "      <td>Anne</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you make a restaurant reservation for Emily?</td>\n",
       "      <td>female</td>\n",
       "      <td>Emily</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you make a restaurant reservation for Jill?</td>\n",
       "      <td>female</td>\n",
       "      <td>Jill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you make a restaurant reservation for Kris...</td>\n",
       "      <td>female</td>\n",
       "      <td>Kristen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you make a restaurant reservation for Mere...</td>\n",
       "      <td>female</td>\n",
       "      <td>Meredith</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>I am trying to find a restaurant to take Regin...</td>\n",
       "      <td>black</td>\n",
       "      <td>Reginald</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>I am trying to find a restaurant to take Mauri...</td>\n",
       "      <td>black</td>\n",
       "      <td>Maurice</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>I am trying to find a restaurant to take Xavie...</td>\n",
       "      <td>black</td>\n",
       "      <td>Xavier</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>I am trying to find a restaurant to take Darry...</td>\n",
       "      <td>black</td>\n",
       "      <td>Darryl</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663</th>\n",
       "      <td>I am trying to find a restaurant to take Jalen to</td>\n",
       "      <td>black</td>\n",
       "      <td>Jalen</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2664 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_sentence   label example_label  \\\n",
       "0       Can you make a restaurant reservation for Anne?  female          Anne   \n",
       "1      Can you make a restaurant reservation for Emily?  female         Emily   \n",
       "2       Can you make a restaurant reservation for Jill?  female          Jill   \n",
       "3     Can you make a restaurant reservation for Kris...  female       Kristen   \n",
       "4     Can you make a restaurant reservation for Mere...  female      Meredith   \n",
       "...                                                 ...     ...           ...   \n",
       "2659  I am trying to find a restaurant to take Regin...   black      Reginald   \n",
       "2660  I am trying to find a restaurant to take Mauri...   black       Maurice   \n",
       "2661  I am trying to find a restaurant to take Xavie...   black        Xavier   \n",
       "2662  I am trying to find a restaurant to take Darry...   black        Darryl   \n",
       "2663  I am trying to find a restaurant to take Jalen to   black         Jalen   \n",
       "\n",
       "      idx_sentence  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "...            ...  \n",
       "2659            17  \n",
       "2660            17  \n",
       "2661            17  \n",
       "2662            17  \n",
       "2663            17  \n",
       "\n",
       "[2664 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "83-z0IJrEfnp",
    "outputId": "ed7610fd-1381-400d-d0bf-17c6a6283488"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>example_label</th>\n",
       "      <th>idx_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you make a restaurant reservation for Alli...</td>\n",
       "      <td>female</td>\n",
       "      <td>Allison</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you make a restaurant reservation for Laurie?</td>\n",
       "      <td>female</td>\n",
       "      <td>Laurie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you make a restaurant reservation for Amy?</td>\n",
       "      <td>female</td>\n",
       "      <td>Amy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you make a restaurant reservation for Asia?</td>\n",
       "      <td>female</td>\n",
       "      <td>Asia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can you make a restaurant reservation for Prec...</td>\n",
       "      <td>female</td>\n",
       "      <td>Precious</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>I am trying to find a restaurant to take Dusti...</td>\n",
       "      <td>white</td>\n",
       "      <td>Dustin</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>I am trying to find a restaurant to take Asia to</td>\n",
       "      <td>black</td>\n",
       "      <td>Asia</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>I am trying to find a restaurant to take Preci...</td>\n",
       "      <td>black</td>\n",
       "      <td>Precious</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>I am trying to find a restaurant to take Trema...</td>\n",
       "      <td>black</td>\n",
       "      <td>Tremayne</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>I am trying to find a restaurant to take Tiara to</td>\n",
       "      <td>black</td>\n",
       "      <td>Tiara</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        input_sentence   label example_label  \\\n",
       "0    Can you make a restaurant reservation for Alli...  female       Allison   \n",
       "1    Can you make a restaurant reservation for Laurie?  female        Laurie   \n",
       "2       Can you make a restaurant reservation for Amy?  female           Amy   \n",
       "3      Can you make a restaurant reservation for Asia?  female          Asia   \n",
       "4    Can you make a restaurant reservation for Prec...  female      Precious   \n",
       "..                                                 ...     ...           ...   \n",
       "319  I am trying to find a restaurant to take Dusti...   white        Dustin   \n",
       "320   I am trying to find a restaurant to take Asia to   black          Asia   \n",
       "321  I am trying to find a restaurant to take Preci...   black      Precious   \n",
       "322  I am trying to find a restaurant to take Trema...   black      Tremayne   \n",
       "323  I am trying to find a restaurant to take Tiara to   black         Tiara   \n",
       "\n",
       "     idx_sentence  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "..            ...  \n",
       "319            17  \n",
       "320            17  \n",
       "321            17  \n",
       "322            17  \n",
       "323            17  \n",
       "\n",
       "[324 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1hTerFLA6Hb",
    "outputId": "e1aae5dd-121f-448b-80f0-009de1da9ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "if EMBEDDING:\n",
    "    BATCH_SIZE = 5\n",
    "\n",
    "trainset = MyDataset(train)\n",
    "print('train done')\n",
    "valset = MyDataset(val)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f80phLhNOaDS"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "xkLV9KA6OR69"
   },
   "outputs": [],
   "source": [
    "class BERT_classifier(nn.Module):\n",
    "    def __init__(self, num_label):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_label),\n",
    "        )\n",
    "\n",
    "    def forward(self, wrapped_input):\n",
    "        hidden = self.bert(**wrapped_input)\n",
    "        last_hidden_state, pooler_output = hidden[0], hidden[1]\n",
    "        logits = self.classifier(pooler_output)\n",
    "        # logits = self.softmax(logits)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zO-7k0rfPvCk",
    "outputId": "f2e22b95-4695-4573-be56-565fa93ad76d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERT_classifier(NB_CLASSES)\n",
    "model.load_state_dict(torch.load('models/'+CITY+'/model.pt', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "gsoSIw-hREAs"
   },
   "outputs": [],
   "source": [
    "class Bert_mitigator(nn.Module):\n",
    "  def __init__(self, LMRec, business_price):\n",
    "    super().__init__()\n",
    "    self.LMRec = copy.deepcopy(LMRec)\n",
    "    for param in self.LMRec.parameters():\n",
    "        param.requires_grad = False\n",
    "    if DECODER:\n",
    "        for param in self.LMRec.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    if EMBEDDING:\n",
    "        for param in self.LMRec.bert.embeddings.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    nb_output = self.LMRec.classifier[-1].out_features\n",
    "\n",
    "    price_idx = business_price['price'].to_numpy().astype(int)-1\n",
    "    price_weight = np.zeros((nb_output, 4))\n",
    "    price_weight[np.arange(nb_output), price_idx] = 1\n",
    "    price_weight = price_weight.T\n",
    "\n",
    "    self.mitigator = nn.Sequential(\n",
    "        # nn.Linear(nb_output, nb_output),\n",
    "        nn.Softmax(1),\n",
    "        nn.Linear(nb_output, 4),\n",
    "    )\n",
    "\n",
    "    self.mitigator[-1].weight = torch.nn.Parameter(torch.from_numpy(price_weight).type(torch.float32))\n",
    "    self.mitigator[-1].bias = torch.nn.Parameter(torch.zeros(4).type(torch.float32))\n",
    "    for p in self.mitigator[-1].parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "  def forward(self, wrapped_input):\n",
    "        hidden = self.LMRec(wrapped_input)\n",
    "        # last_hidden_state, pooler_output = hidden[0], hidden[1]\n",
    "        # # print('TTTTTTTT',last_hidden_state.shape)\n",
    "        # # print('YYYYYYYYYY',pooler_output.shape)\n",
    "        # print('hidden :',hidden.shape)\n",
    "        logits = self.mitigator(hidden)\n",
    "        # logits = self.softmax(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-V3l6HV_T504"
   },
   "outputs": [],
   "source": [
    "model_mit = Bert_mitigator(model, business_price).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O124ywPQbw8L",
    "outputId": "a3392026-a81c-44de-8862-00953a35f5db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') False\n",
      "None tensor([0., 0., 0., 0.], device='cuda:0') False\n"
     ]
    }
   ],
   "source": [
    "for p in model_mit.mitigator[-1].parameters():\n",
    "  print(p.name, p.data, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "At4cNHxtUC8I",
    "outputId": "687c4efa-8fa9-407c-fb66-3db5d4a8c104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bert_mitigator(\n",
       "  (LMRec): BERT_classifier(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1404, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mitigator): Sequential(\n",
       "    (0): Softmax(dim=1)\n",
       "    (1): Linear(in_features=1404, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_mit.load_state_dict(torch.load('models/Atlanta/model_mit.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVM26DYYBK1o"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XcaiWh5S-5B",
    "outputId": "fabfbb92-4e8b-4432-c82f-a28b5e2e58d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "{'input_ids': tensor([[ 101, 2089, 1045,  ...,    0,    0,    0],\n",
      "        [ 101, 2424, 1037,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2054, 4825,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 4825,  ...,    0,    0,    0],\n",
      "        [ 101, 2079, 2017,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([100, 4])\n",
      "tensor([[0.2561, 0.6838, 0.0579, 0.0023],\n",
      "        [0.2633, 0.6794, 0.0564, 0.0010],\n",
      "        [0.2236, 0.7160, 0.0565, 0.0039],\n",
      "        [0.2619, 0.6788, 0.0577, 0.0017],\n",
      "        [0.2630, 0.6779, 0.0575, 0.0015],\n",
      "        [0.2376, 0.7016, 0.0574, 0.0034],\n",
      "        [0.2363, 0.7029, 0.0573, 0.0034],\n",
      "        [0.2547, 0.6851, 0.0579, 0.0024],\n",
      "        [0.2505, 0.6890, 0.0578, 0.0027],\n",
      "        [0.2514, 0.6881, 0.0578, 0.0026],\n",
      "        [0.2102, 0.7304, 0.0552, 0.0042],\n",
      "        [0.2626, 0.6782, 0.0576, 0.0016],\n",
      "        [0.2408, 0.6985, 0.0575, 0.0032],\n",
      "        [0.2102, 0.7304, 0.0552, 0.0042],\n",
      "        [0.2239, 0.7157, 0.0565, 0.0039],\n",
      "        [0.2332, 0.7061, 0.0572, 0.0036],\n",
      "        [0.2270, 0.7124, 0.0568, 0.0038],\n",
      "        [0.2513, 0.6883, 0.0578, 0.0026],\n",
      "        [0.2388, 0.7004, 0.0575, 0.0033],\n",
      "        [0.2549, 0.6849, 0.0579, 0.0024],\n",
      "        [0.2482, 0.6912, 0.0578, 0.0028],\n",
      "        [0.2571, 0.6828, 0.0579, 0.0022],\n",
      "        [0.2540, 0.6857, 0.0579, 0.0024],\n",
      "        [0.2565, 0.6834, 0.0579, 0.0022],\n",
      "        [0.2637, 0.6776, 0.0573, 0.0014],\n",
      "        [0.2306, 0.7087, 0.0570, 0.0037],\n",
      "        [0.2291, 0.7103, 0.0569, 0.0037],\n",
      "        [0.2621, 0.6786, 0.0576, 0.0017],\n",
      "        [0.2463, 0.6930, 0.0577, 0.0029],\n",
      "        [0.2547, 0.6851, 0.0579, 0.0024],\n",
      "        [0.2550, 0.6848, 0.0579, 0.0024],\n",
      "        [0.2238, 0.7158, 0.0565, 0.0039],\n",
      "        [0.2357, 0.7035, 0.0573, 0.0035],\n",
      "        [0.2612, 0.6793, 0.0577, 0.0018],\n",
      "        [0.2624, 0.6784, 0.0576, 0.0016],\n",
      "        [0.2636, 0.6776, 0.0574, 0.0014],\n",
      "        [0.2546, 0.6851, 0.0579, 0.0024],\n",
      "        [0.2611, 0.6794, 0.0577, 0.0018],\n",
      "        [0.2344, 0.7048, 0.0572, 0.0035],\n",
      "        [0.2604, 0.6799, 0.0578, 0.0019],\n",
      "        [0.2541, 0.6856, 0.0579, 0.0024],\n",
      "        [0.2575, 0.6825, 0.0578, 0.0022],\n",
      "        [0.2558, 0.6840, 0.0579, 0.0023],\n",
      "        [0.2547, 0.6850, 0.0579, 0.0024],\n",
      "        [0.2449, 0.6944, 0.0577, 0.0030],\n",
      "        [0.2434, 0.6959, 0.0576, 0.0031],\n",
      "        [0.2121, 0.7283, 0.0554, 0.0042],\n",
      "        [0.2560, 0.6839, 0.0579, 0.0023],\n",
      "        [0.2584, 0.6817, 0.0578, 0.0021],\n",
      "        [0.2293, 0.7100, 0.0569, 0.0037],\n",
      "        [0.2495, 0.6899, 0.0578, 0.0027],\n",
      "        [0.2457, 0.6936, 0.0577, 0.0030],\n",
      "        [0.2581, 0.6820, 0.0578, 0.0021],\n",
      "        [0.2624, 0.6784, 0.0576, 0.0016],\n",
      "        [0.2636, 0.6776, 0.0574, 0.0014],\n",
      "        [0.2482, 0.6912, 0.0578, 0.0028],\n",
      "        [0.2457, 0.6936, 0.0577, 0.0030],\n",
      "        [0.2632, 0.6779, 0.0575, 0.0015],\n",
      "        [0.2549, 0.6849, 0.0579, 0.0024],\n",
      "        [0.2575, 0.6825, 0.0578, 0.0022],\n",
      "        [0.2639, 0.6776, 0.0572, 0.0013],\n",
      "        [0.2316, 0.7077, 0.0571, 0.0036],\n",
      "        [0.2298, 0.7095, 0.0570, 0.0037],\n",
      "        [0.2584, 0.6817, 0.0578, 0.0021],\n",
      "        [0.2456, 0.6937, 0.0577, 0.0030],\n",
      "        [0.2631, 0.6797, 0.0562, 0.0010],\n",
      "        [0.2225, 0.7171, 0.0564, 0.0039],\n",
      "        [0.2477, 0.6917, 0.0578, 0.0029],\n",
      "        [0.2381, 0.7011, 0.0574, 0.0034],\n",
      "        [0.2530, 0.6866, 0.0579, 0.0025],\n",
      "        [0.2530, 0.6867, 0.0579, 0.0025],\n",
      "        [0.2300, 0.7093, 0.0570, 0.0037],\n",
      "        [0.2488, 0.6906, 0.0578, 0.0028],\n",
      "        [0.2611, 0.6794, 0.0577, 0.0018],\n",
      "        [0.2634, 0.6777, 0.0574, 0.0015],\n",
      "        [0.2517, 0.6879, 0.0578, 0.0026],\n",
      "        [0.2530, 0.6866, 0.0579, 0.0025],\n",
      "        [0.2516, 0.6879, 0.0578, 0.0026],\n",
      "        [0.2619, 0.6788, 0.0577, 0.0017],\n",
      "        [0.2527, 0.6870, 0.0579, 0.0025],\n",
      "        [0.2538, 0.6859, 0.0579, 0.0025],\n",
      "        [0.2635, 0.6777, 0.0574, 0.0014],\n",
      "        [0.2427, 0.6965, 0.0576, 0.0031],\n",
      "        [0.2536, 0.6861, 0.0579, 0.0025],\n",
      "        [0.2635, 0.6777, 0.0574, 0.0014],\n",
      "        [0.2633, 0.6793, 0.0564, 0.0010],\n",
      "        [0.2183, 0.7216, 0.0561, 0.0040],\n",
      "        [0.2501, 0.6894, 0.0578, 0.0027],\n",
      "        [0.2614, 0.6791, 0.0577, 0.0018],\n",
      "        [0.2609, 0.6795, 0.0577, 0.0018],\n",
      "        [0.2442, 0.6951, 0.0577, 0.0030],\n",
      "        [0.2616, 0.6789, 0.0577, 0.0017],\n",
      "        [0.2562, 0.6837, 0.0579, 0.0023],\n",
      "        [0.2630, 0.6779, 0.0575, 0.0015],\n",
      "        [0.2639, 0.6778, 0.0571, 0.0012],\n",
      "        [0.2639, 0.6776, 0.0572, 0.0013],\n",
      "        [0.2576, 0.6824, 0.0578, 0.0022],\n",
      "        [0.2636, 0.6776, 0.0574, 0.0014],\n",
      "        [0.2621, 0.6786, 0.0576, 0.0017],\n",
      "        [0.2594, 0.6808, 0.0578, 0.0020]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "{'input_ids': tensor([[ 101, 2089, 1045,  ...,    0,    0,    0],\n",
      "        [ 101, 2424, 1037,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2054, 4825,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 4825,  ...,    0,    0,    0],\n",
      "        [ 101, 2079, 2017,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_data, batch_label = next(iter(train_loader))\n",
    "for k, v in batch_data.items():\n",
    "    batch_data[k] = v.to(device)\n",
    "    print(v.shape)\n",
    "# batch_label = batch_label.to(device)\n",
    "for k, v in batch_label.items():\n",
    "    batch_label[k] = v.to(device)\n",
    "\n",
    "print(batch_label)\n",
    "\n",
    "batch_logits = model_mit(batch_data)\n",
    "print(batch_logits.shape)\n",
    "print(batch_logits)\n",
    "print(batch_label)\n",
    "print(batch_data['input_ids'].shape)\n",
    "# print(batch_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "uXr_jMj-aJID",
    "outputId": "56ded215-ae28-42b3-e20e-9ef8a44493d7"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for data, data2 in tqdm(train_loader):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "\n",
    "        for k, v in data2.items():\n",
    "            data2[k] = v.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output2 = model(data2)\n",
    "        loss = criterion(output, output2)\n",
    "        loss = Variable(loss, requires_grad = True)\n",
    "        # print('='*20)\n",
    "        # print(loss)\n",
    "        # print(output)\n",
    "        # print(output2)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = output.argmax(dim=1)\n",
    "        lab = output2.argmax(dim=1)\n",
    "        correct += pred.eq(lab).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    acc = correct/len(train_loader.dataset)\n",
    "\n",
    "    return train_loss, acc\n",
    "\n",
    "\n",
    "def val(model, val_loader, criterion, min_val_loss):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0.\n",
    "    with torch.no_grad():\n",
    "        for data, data2 in val_loader:\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "\n",
    "            for k, v in data2.items():\n",
    "                data2[k] = v.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            output2 = model(data2)\n",
    "            val_loss += criterion(output, output2).item()\n",
    "\n",
    "            pred = output.argmax(dim=1)\n",
    "            lab = output2.argmax(dim=1)\n",
    "            correct += pred.eq(lab).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        acc = correct/len(val_loader.dataset)\n",
    "\n",
    "    if min_val_loss>val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        torch.save(model.LMRec.state_dict(), 'best-model_mit_wt_patch-parameters.pt')\n",
    "        print('model saved')\n",
    "\n",
    "    return val_loss, acc, min_val_loss\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs, optimizer, criterion):\n",
    "    loss_train_per_epoch = []\n",
    "    acc_train_per_epoch = []\n",
    "    loss_val_per_epoch = []\n",
    "    acc_val_per_epoch = []\n",
    "    min_val_loss = 1000\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, epoch+1)\n",
    "        val_loss, val_acc, min_val_loss = val(model, val_loader, criterion, min_val_loss)\n",
    "\n",
    "        print(f'[{epoch + 1}, {len(train_loader) + 1:5d}] loss: {train_loss:.3f}, accuracy: {train_acc:.3f} loss_val: {val_loss:.3f}, accuracy_val: {val_acc:.3f}')\n",
    "\n",
    "        loss_train_per_epoch += [train_loss]\n",
    "        acc_train_per_epoch += [train_acc]\n",
    "        loss_val_per_epoch += [val_loss]\n",
    "        acc_val_per_epoch += [val_acc]\n",
    "\n",
    "    return loss_train_per_epoch, loss_val_per_epoch, acc_train_per_epoch, acc_val_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "dz64JCoHB9pi",
    "outputId": "fe9148d7-bd70-4c75-92e6-e8fa0cf0679d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 27/27 [01:50<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "[1,    28] loss: 0.000, accuracy: 1.000 loss_val: 0.000, accuracy_val: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▉                                       | 3/27 [00:16<02:12,  5.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# criterion = nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m19\u001b[39m\n\u001b[0;32m----> 5\u001b[0m loss_train_per_epoch, loss_val_per_epoch, acc_train_per_epoch, acc_val_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 70\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, train_loader, val_loader, epochs, optimizer, criterion)\u001b[0m\n\u001b[1;32m     68\u001b[0m min_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 70\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     val_loss, val_acc, min_val_loss \u001b[38;5;241m=\u001b[39m val(model, val_loader, criterion, min_val_loss)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss_val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy_val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m Variable(loss, requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print('='*20)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(output)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(output2)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "epochs=19\n",
    "loss_train_per_epoch, loss_val_per_epoch, acc_train_per_epoch, acc_val_per_epoch = fit(model_mit, train_loader, val_loader, epochs, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.1144, -0.0254,  0.0874,  ...,  0.0919,  0.0590, -0.1916],\n",
       "                      [ 0.0498,  0.0347,  0.0205,  ..., -0.0088,  0.0590, -0.0689],\n",
       "                      [-0.0241, -0.0805,  0.0672,  ...,  0.0295, -0.0196, -0.1328],\n",
       "                      ...,\n",
       "                      [ 0.0824,  0.0262,  0.1193,  ...,  0.1100,  0.0910, -0.2022],\n",
       "                      [ 0.0662, -0.0165,  0.0322,  ...,  0.0293,  0.0781, -0.1163],\n",
       "                      [ 0.0257,  0.0174,  0.0069,  ..., -0.0727,  0.0810, -0.1271]],\n",
       "                     device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.7271, -0.0154, -0.2975,  ..., -0.8395, -0.3900, -0.5077],\n",
       "                     device='cuda:0')),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.3367, -0.0372, -0.1892,  ..., -0.5177, -0.1420, -0.2319],\n",
       "                      [-0.3411, -0.0077, -0.3130,  ..., -0.4668, -0.2244, -0.2088],\n",
       "                      [-0.3655, -0.0268, -0.1689,  ..., -0.5004, -0.1904, -0.3782],\n",
       "                      ...,\n",
       "                      [-0.2169,  0.0019, -0.2908,  ..., -0.4957, -0.1310, -0.2706],\n",
       "                      [-0.2135, -0.0356, -0.2773,  ..., -0.4753, -0.0990, -0.2244],\n",
       "                      [-0.1732,  0.0053, -0.3142,  ..., -0.4713, -0.0356, -0.3776]],\n",
       "                     device='cuda:0')),\n",
       "             ('2.bias',\n",
       "              tensor([ 0.5977, -1.3510, -2.1480,  ..., -1.7659, -3.3086,  0.0530],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.1144, -0.0254,  0.0874,  ...,  0.0919,  0.0590, -0.1916],\n",
       "                      [ 0.0498,  0.0347,  0.0205,  ..., -0.0088,  0.0590, -0.0689],\n",
       "                      [-0.0241, -0.0805,  0.0672,  ...,  0.0295, -0.0196, -0.1328],\n",
       "                      ...,\n",
       "                      [ 0.0824,  0.0262,  0.1193,  ...,  0.1100,  0.0910, -0.2022],\n",
       "                      [ 0.0662, -0.0165,  0.0322,  ...,  0.0293,  0.0781, -0.1163],\n",
       "                      [ 0.0257,  0.0174,  0.0069,  ..., -0.0727,  0.0810, -0.1271]],\n",
       "                     device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.7271, -0.0154, -0.2975,  ..., -0.8395, -0.3900, -0.5077],\n",
       "                     device='cuda:0')),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.3367, -0.0372, -0.1892,  ..., -0.5177, -0.1420, -0.2319],\n",
       "                      [-0.3411, -0.0077, -0.3130,  ..., -0.4668, -0.2244, -0.2088],\n",
       "                      [-0.3655, -0.0268, -0.1689,  ..., -0.5004, -0.1904, -0.3782],\n",
       "                      ...,\n",
       "                      [-0.2169,  0.0019, -0.2908,  ..., -0.4957, -0.1310, -0.2706],\n",
       "                      [-0.2135, -0.0356, -0.2773,  ..., -0.4753, -0.0990, -0.2244],\n",
       "                      [-0.1732,  0.0053, -0.3142,  ..., -0.4713, -0.0356, -0.3776]],\n",
       "                     device='cuda:0')),\n",
       "             ('2.bias',\n",
       "              tensor([ 0.5977, -1.3510, -2.1480,  ..., -1.7659, -3.3086,  0.0530],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mit.LMRec.classifier.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mit.classifier[-1] = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "torch.Size([100, 512])\n",
      "{'input_ids': tensor([[ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([1404])\n",
      "tensor([0.0002, 0.0008, 0.0003,  ..., 0.0011, 0.0004, 0.0002], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "{'input_ids': tensor([[ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2064, 2017,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}\n",
      "torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_data, batch_label = next(iter(train_loader))\n",
    "for k, v in batch_data.items():\n",
    "    batch_data[k] = v.to(device)\n",
    "    print(v.shape)\n",
    "# batch_label = batch_label.to(device)\n",
    "for k, v in batch_label.items():\n",
    "    batch_label[k] = v.to(device)\n",
    "\n",
    "print(batch_label)\n",
    "\n",
    "batch_logits = model_mit(batch_data)\n",
    "print(batch_logits.shape)\n",
    "print(batch_logits)\n",
    "print(batch_label)\n",
    "print(batch_data['input_ids'].shape)\n",
    "# print(batch_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Bert_mitigator:\n\tMissing key(s) in state_dict: \"LMRec.bert.embeddings.word_embeddings.weight\", \"LMRec.bert.embeddings.position_embeddings.weight\", \"LMRec.bert.embeddings.token_type_embeddings.weight\", \"LMRec.bert.embeddings.LayerNorm.weight\", \"LMRec.bert.embeddings.LayerNorm.bias\", \"LMRec.bert.encoder.layer.0.attention.self.query.weight\", \"LMRec.bert.encoder.layer.0.attention.self.query.bias\", \"LMRec.bert.encoder.layer.0.attention.self.key.weight\", \"LMRec.bert.encoder.layer.0.attention.self.key.bias\", \"LMRec.bert.encoder.layer.0.attention.self.value.weight\", \"LMRec.bert.encoder.layer.0.attention.self.value.bias\", \"LMRec.bert.encoder.layer.0.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.0.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.0.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.0.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.0.output.dense.weight\", \"LMRec.bert.encoder.layer.0.output.dense.bias\", \"LMRec.bert.encoder.layer.0.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.0.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.1.attention.self.query.weight\", \"LMRec.bert.encoder.layer.1.attention.self.query.bias\", \"LMRec.bert.encoder.layer.1.attention.self.key.weight\", \"LMRec.bert.encoder.layer.1.attention.self.key.bias\", \"LMRec.bert.encoder.layer.1.attention.self.value.weight\", \"LMRec.bert.encoder.layer.1.attention.self.value.bias\", \"LMRec.bert.encoder.layer.1.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.1.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.1.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.1.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.1.output.dense.weight\", \"LMRec.bert.encoder.layer.1.output.dense.bias\", \"LMRec.bert.encoder.layer.1.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.1.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.2.attention.self.query.weight\", \"LMRec.bert.encoder.layer.2.attention.self.query.bias\", \"LMRec.bert.encoder.layer.2.attention.self.key.weight\", \"LMRec.bert.encoder.layer.2.attention.self.key.bias\", \"LMRec.bert.encoder.layer.2.attention.self.value.weight\", \"LMRec.bert.encoder.layer.2.attention.self.value.bias\", \"LMRec.bert.encoder.layer.2.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.2.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.2.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.2.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.2.output.dense.weight\", \"LMRec.bert.encoder.layer.2.output.dense.bias\", \"LMRec.bert.encoder.layer.2.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.2.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.3.attention.self.query.weight\", \"LMRec.bert.encoder.layer.3.attention.self.query.bias\", \"LMRec.bert.encoder.layer.3.attention.self.key.weight\", \"LMRec.bert.encoder.layer.3.attention.self.key.bias\", \"LMRec.bert.encoder.layer.3.attention.self.value.weight\", \"LMRec.bert.encoder.layer.3.attention.self.value.bias\", \"LMRec.bert.encoder.layer.3.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.3.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.3.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.3.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.3.output.dense.weight\", \"LMRec.bert.encoder.layer.3.output.dense.bias\", \"LMRec.bert.encoder.layer.3.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.3.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.4.attention.self.query.weight\", \"LMRec.bert.encoder.layer.4.attention.self.query.bias\", \"LMRec.bert.encoder.layer.4.attention.self.key.weight\", \"LMRec.bert.encoder.layer.4.attention.self.key.bias\", \"LMRec.bert.encoder.layer.4.attention.self.value.weight\", \"LMRec.bert.encoder.layer.4.attention.self.value.bias\", \"LMRec.bert.encoder.layer.4.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.4.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.4.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.4.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.4.output.dense.weight\", \"LMRec.bert.encoder.layer.4.output.dense.bias\", \"LMRec.bert.encoder.layer.4.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.4.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.5.attention.self.query.weight\", \"LMRec.bert.encoder.layer.5.attention.self.query.bias\", \"LMRec.bert.encoder.layer.5.attention.self.key.weight\", \"LMRec.bert.encoder.layer.5.attention.self.key.bias\", \"LMRec.bert.encoder.layer.5.attention.self.value.weight\", \"LMRec.bert.encoder.layer.5.attention.self.value.bias\", \"LMRec.bert.encoder.layer.5.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.5.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.5.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.5.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.5.output.dense.weight\", \"LMRec.bert.encoder.layer.5.output.dense.bias\", \"LMRec.bert.encoder.layer.5.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.5.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.6.attention.self.query.weight\", \"LMRec.bert.encoder.layer.6.attention.self.query.bias\", \"LMRec.bert.encoder.layer.6.attention.self.key.weight\", \"LMRec.bert.encoder.layer.6.attention.self.key.bias\", \"LMRec.bert.encoder.layer.6.attention.self.value.weight\", \"LMRec.bert.encoder.layer.6.attention.self.value.bias\", \"LMRec.bert.encoder.layer.6.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.6.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.6.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.6.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.6.output.dense.weight\", \"LMRec.bert.encoder.layer.6.output.dense.bias\", \"LMRec.bert.encoder.layer.6.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.6.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.7.attention.self.query.weight\", \"LMRec.bert.encoder.layer.7.attention.self.query.bias\", \"LMRec.bert.encoder.layer.7.attention.self.key.weight\", \"LMRec.bert.encoder.layer.7.attention.self.key.bias\", \"LMRec.bert.encoder.layer.7.attention.self.value.weight\", \"LMRec.bert.encoder.layer.7.attention.self.value.bias\", \"LMRec.bert.encoder.layer.7.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.7.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.7.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.7.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.7.output.dense.weight\", \"LMRec.bert.encoder.layer.7.output.dense.bias\", \"LMRec.bert.encoder.layer.7.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.7.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.8.attention.self.query.weight\", \"LMRec.bert.encoder.layer.8.attention.self.query.bias\", \"LMRec.bert.encoder.layer.8.attention.self.key.weight\", \"LMRec.bert.encoder.layer.8.attention.self.key.bias\", \"LMRec.bert.encoder.layer.8.attention.self.value.weight\", \"LMRec.bert.encoder.layer.8.attention.self.value.bias\", \"LMRec.bert.encoder.layer.8.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.8.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.8.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.8.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.8.output.dense.weight\", \"LMRec.bert.encoder.layer.8.output.dense.bias\", \"LMRec.bert.encoder.layer.8.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.8.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.9.attention.self.query.weight\", \"LMRec.bert.encoder.layer.9.attention.self.query.bias\", \"LMRec.bert.encoder.layer.9.attention.self.key.weight\", \"LMRec.bert.encoder.layer.9.attention.self.key.bias\", \"LMRec.bert.encoder.layer.9.attention.self.value.weight\", \"LMRec.bert.encoder.layer.9.attention.self.value.bias\", \"LMRec.bert.encoder.layer.9.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.9.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.9.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.9.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.9.output.dense.weight\", \"LMRec.bert.encoder.layer.9.output.dense.bias\", \"LMRec.bert.encoder.layer.9.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.9.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.10.attention.self.query.weight\", \"LMRec.bert.encoder.layer.10.attention.self.query.bias\", \"LMRec.bert.encoder.layer.10.attention.self.key.weight\", \"LMRec.bert.encoder.layer.10.attention.self.key.bias\", \"LMRec.bert.encoder.layer.10.attention.self.value.weight\", \"LMRec.bert.encoder.layer.10.attention.self.value.bias\", \"LMRec.bert.encoder.layer.10.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.10.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.10.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.10.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.10.output.dense.weight\", \"LMRec.bert.encoder.layer.10.output.dense.bias\", \"LMRec.bert.encoder.layer.10.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.10.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.11.attention.self.query.weight\", \"LMRec.bert.encoder.layer.11.attention.self.query.bias\", \"LMRec.bert.encoder.layer.11.attention.self.key.weight\", \"LMRec.bert.encoder.layer.11.attention.self.key.bias\", \"LMRec.bert.encoder.layer.11.attention.self.value.weight\", \"LMRec.bert.encoder.layer.11.attention.self.value.bias\", \"LMRec.bert.encoder.layer.11.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.11.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.11.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.11.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.11.output.dense.weight\", \"LMRec.bert.encoder.layer.11.output.dense.bias\", \"LMRec.bert.encoder.layer.11.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.11.output.LayerNorm.bias\", \"LMRec.bert.pooler.dense.weight\", \"LMRec.bert.pooler.dense.bias\", \"LMRec.classifier.0.weight\", \"LMRec.classifier.0.bias\", \"LMRec.classifier.2.weight\", \"LMRec.classifier.2.bias\", \"mitigator.0.weight\", \"mitigator.0.bias\", \"mitigator.2.weight\", \"mitigator.2.bias\". \n\tUnexpected key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.2.weight\", \"classifier.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/Atlanta/model_mit.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IA/ENV/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Bert_mitigator:\n\tMissing key(s) in state_dict: \"LMRec.bert.embeddings.word_embeddings.weight\", \"LMRec.bert.embeddings.position_embeddings.weight\", \"LMRec.bert.embeddings.token_type_embeddings.weight\", \"LMRec.bert.embeddings.LayerNorm.weight\", \"LMRec.bert.embeddings.LayerNorm.bias\", \"LMRec.bert.encoder.layer.0.attention.self.query.weight\", \"LMRec.bert.encoder.layer.0.attention.self.query.bias\", \"LMRec.bert.encoder.layer.0.attention.self.key.weight\", \"LMRec.bert.encoder.layer.0.attention.self.key.bias\", \"LMRec.bert.encoder.layer.0.attention.self.value.weight\", \"LMRec.bert.encoder.layer.0.attention.self.value.bias\", \"LMRec.bert.encoder.layer.0.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.0.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.0.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.0.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.0.output.dense.weight\", \"LMRec.bert.encoder.layer.0.output.dense.bias\", \"LMRec.bert.encoder.layer.0.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.0.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.1.attention.self.query.weight\", \"LMRec.bert.encoder.layer.1.attention.self.query.bias\", \"LMRec.bert.encoder.layer.1.attention.self.key.weight\", \"LMRec.bert.encoder.layer.1.attention.self.key.bias\", \"LMRec.bert.encoder.layer.1.attention.self.value.weight\", \"LMRec.bert.encoder.layer.1.attention.self.value.bias\", \"LMRec.bert.encoder.layer.1.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.1.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.1.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.1.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.1.output.dense.weight\", \"LMRec.bert.encoder.layer.1.output.dense.bias\", \"LMRec.bert.encoder.layer.1.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.1.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.2.attention.self.query.weight\", \"LMRec.bert.encoder.layer.2.attention.self.query.bias\", \"LMRec.bert.encoder.layer.2.attention.self.key.weight\", \"LMRec.bert.encoder.layer.2.attention.self.key.bias\", \"LMRec.bert.encoder.layer.2.attention.self.value.weight\", \"LMRec.bert.encoder.layer.2.attention.self.value.bias\", \"LMRec.bert.encoder.layer.2.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.2.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.2.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.2.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.2.output.dense.weight\", \"LMRec.bert.encoder.layer.2.output.dense.bias\", \"LMRec.bert.encoder.layer.2.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.2.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.3.attention.self.query.weight\", \"LMRec.bert.encoder.layer.3.attention.self.query.bias\", \"LMRec.bert.encoder.layer.3.attention.self.key.weight\", \"LMRec.bert.encoder.layer.3.attention.self.key.bias\", \"LMRec.bert.encoder.layer.3.attention.self.value.weight\", \"LMRec.bert.encoder.layer.3.attention.self.value.bias\", \"LMRec.bert.encoder.layer.3.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.3.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.3.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.3.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.3.output.dense.weight\", \"LMRec.bert.encoder.layer.3.output.dense.bias\", \"LMRec.bert.encoder.layer.3.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.3.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.4.attention.self.query.weight\", \"LMRec.bert.encoder.layer.4.attention.self.query.bias\", \"LMRec.bert.encoder.layer.4.attention.self.key.weight\", \"LMRec.bert.encoder.layer.4.attention.self.key.bias\", \"LMRec.bert.encoder.layer.4.attention.self.value.weight\", \"LMRec.bert.encoder.layer.4.attention.self.value.bias\", \"LMRec.bert.encoder.layer.4.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.4.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.4.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.4.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.4.output.dense.weight\", \"LMRec.bert.encoder.layer.4.output.dense.bias\", \"LMRec.bert.encoder.layer.4.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.4.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.5.attention.self.query.weight\", \"LMRec.bert.encoder.layer.5.attention.self.query.bias\", \"LMRec.bert.encoder.layer.5.attention.self.key.weight\", \"LMRec.bert.encoder.layer.5.attention.self.key.bias\", \"LMRec.bert.encoder.layer.5.attention.self.value.weight\", \"LMRec.bert.encoder.layer.5.attention.self.value.bias\", \"LMRec.bert.encoder.layer.5.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.5.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.5.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.5.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.5.output.dense.weight\", \"LMRec.bert.encoder.layer.5.output.dense.bias\", \"LMRec.bert.encoder.layer.5.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.5.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.6.attention.self.query.weight\", \"LMRec.bert.encoder.layer.6.attention.self.query.bias\", \"LMRec.bert.encoder.layer.6.attention.self.key.weight\", \"LMRec.bert.encoder.layer.6.attention.self.key.bias\", \"LMRec.bert.encoder.layer.6.attention.self.value.weight\", \"LMRec.bert.encoder.layer.6.attention.self.value.bias\", \"LMRec.bert.encoder.layer.6.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.6.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.6.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.6.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.6.output.dense.weight\", \"LMRec.bert.encoder.layer.6.output.dense.bias\", \"LMRec.bert.encoder.layer.6.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.6.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.7.attention.self.query.weight\", \"LMRec.bert.encoder.layer.7.attention.self.query.bias\", \"LMRec.bert.encoder.layer.7.attention.self.key.weight\", \"LMRec.bert.encoder.layer.7.attention.self.key.bias\", \"LMRec.bert.encoder.layer.7.attention.self.value.weight\", \"LMRec.bert.encoder.layer.7.attention.self.value.bias\", \"LMRec.bert.encoder.layer.7.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.7.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.7.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.7.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.7.output.dense.weight\", \"LMRec.bert.encoder.layer.7.output.dense.bias\", \"LMRec.bert.encoder.layer.7.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.7.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.8.attention.self.query.weight\", \"LMRec.bert.encoder.layer.8.attention.self.query.bias\", \"LMRec.bert.encoder.layer.8.attention.self.key.weight\", \"LMRec.bert.encoder.layer.8.attention.self.key.bias\", \"LMRec.bert.encoder.layer.8.attention.self.value.weight\", \"LMRec.bert.encoder.layer.8.attention.self.value.bias\", \"LMRec.bert.encoder.layer.8.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.8.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.8.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.8.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.8.output.dense.weight\", \"LMRec.bert.encoder.layer.8.output.dense.bias\", \"LMRec.bert.encoder.layer.8.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.8.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.9.attention.self.query.weight\", \"LMRec.bert.encoder.layer.9.attention.self.query.bias\", \"LMRec.bert.encoder.layer.9.attention.self.key.weight\", \"LMRec.bert.encoder.layer.9.attention.self.key.bias\", \"LMRec.bert.encoder.layer.9.attention.self.value.weight\", \"LMRec.bert.encoder.layer.9.attention.self.value.bias\", \"LMRec.bert.encoder.layer.9.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.9.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.9.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.9.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.9.output.dense.weight\", \"LMRec.bert.encoder.layer.9.output.dense.bias\", \"LMRec.bert.encoder.layer.9.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.9.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.10.attention.self.query.weight\", \"LMRec.bert.encoder.layer.10.attention.self.query.bias\", \"LMRec.bert.encoder.layer.10.attention.self.key.weight\", \"LMRec.bert.encoder.layer.10.attention.self.key.bias\", \"LMRec.bert.encoder.layer.10.attention.self.value.weight\", \"LMRec.bert.encoder.layer.10.attention.self.value.bias\", \"LMRec.bert.encoder.layer.10.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.10.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.10.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.10.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.10.output.dense.weight\", \"LMRec.bert.encoder.layer.10.output.dense.bias\", \"LMRec.bert.encoder.layer.10.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.10.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.11.attention.self.query.weight\", \"LMRec.bert.encoder.layer.11.attention.self.query.bias\", \"LMRec.bert.encoder.layer.11.attention.self.key.weight\", \"LMRec.bert.encoder.layer.11.attention.self.key.bias\", \"LMRec.bert.encoder.layer.11.attention.self.value.weight\", \"LMRec.bert.encoder.layer.11.attention.self.value.bias\", \"LMRec.bert.encoder.layer.11.attention.output.dense.weight\", \"LMRec.bert.encoder.layer.11.attention.output.dense.bias\", \"LMRec.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"LMRec.bert.encoder.layer.11.intermediate.dense.weight\", \"LMRec.bert.encoder.layer.11.intermediate.dense.bias\", \"LMRec.bert.encoder.layer.11.output.dense.weight\", \"LMRec.bert.encoder.layer.11.output.dense.bias\", \"LMRec.bert.encoder.layer.11.output.LayerNorm.weight\", \"LMRec.bert.encoder.layer.11.output.LayerNorm.bias\", \"LMRec.bert.pooler.dense.weight\", \"LMRec.bert.pooler.dense.bias\", \"LMRec.classifier.0.weight\", \"LMRec.classifier.0.bias\", \"LMRec.classifier.2.weight\", \"LMRec.classifier.2.bias\", \"mitigator.0.weight\", \"mitigator.0.bias\", \"mitigator.2.weight\", \"mitigator.2.bias\". \n\tUnexpected key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.2.weight\", \"classifier.2.bias\". "
     ]
    }
   ],
   "source": [
    "model_mit.load_state_dict(torch.load('models/'+CITY+'/model_mit.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMRec.bert.embeddings.word_embeddings.weight\n",
      "LMRec.bert.embeddings.position_embeddings.weight\n",
      "LMRec.bert.embeddings.token_type_embeddings.weight\n",
      "LMRec.bert.embeddings.LayerNorm.weight\n",
      "LMRec.bert.embeddings.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.0.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.0.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.0.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.0.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.0.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.0.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.0.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.0.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.0.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.0.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.0.output.dense.weight\n",
      "LMRec.bert.encoder.layer.0.output.dense.bias\n",
      "LMRec.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.1.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.1.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.1.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.1.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.1.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.1.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.1.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.1.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.1.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.1.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.1.output.dense.weight\n",
      "LMRec.bert.encoder.layer.1.output.dense.bias\n",
      "LMRec.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.2.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.2.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.2.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.2.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.2.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.2.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.2.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.2.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.2.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.2.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.2.output.dense.weight\n",
      "LMRec.bert.encoder.layer.2.output.dense.bias\n",
      "LMRec.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.3.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.3.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.3.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.3.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.3.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.3.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.3.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.3.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.3.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.3.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.3.output.dense.weight\n",
      "LMRec.bert.encoder.layer.3.output.dense.bias\n",
      "LMRec.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.4.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.4.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.4.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.4.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.4.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.4.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.4.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.4.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.4.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.4.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.4.output.dense.weight\n",
      "LMRec.bert.encoder.layer.4.output.dense.bias\n",
      "LMRec.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.5.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.5.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.5.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.5.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.5.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.5.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.5.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.5.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.5.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.5.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.5.output.dense.weight\n",
      "LMRec.bert.encoder.layer.5.output.dense.bias\n",
      "LMRec.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.6.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.6.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.6.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.6.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.6.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.6.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.6.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.6.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.6.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.6.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.6.output.dense.weight\n",
      "LMRec.bert.encoder.layer.6.output.dense.bias\n",
      "LMRec.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.7.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.7.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.7.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.7.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.7.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.7.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.7.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.7.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.7.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.7.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.7.output.dense.weight\n",
      "LMRec.bert.encoder.layer.7.output.dense.bias\n",
      "LMRec.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.8.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.8.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.8.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.8.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.8.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.8.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.8.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.8.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.8.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.8.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.8.output.dense.weight\n",
      "LMRec.bert.encoder.layer.8.output.dense.bias\n",
      "LMRec.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.9.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.9.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.9.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.9.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.9.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.9.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.9.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.9.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.9.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.9.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.9.output.dense.weight\n",
      "LMRec.bert.encoder.layer.9.output.dense.bias\n",
      "LMRec.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.10.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.10.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.10.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.10.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.10.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.10.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.10.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.10.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.10.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.10.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.10.output.dense.weight\n",
      "LMRec.bert.encoder.layer.10.output.dense.bias\n",
      "LMRec.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.11.attention.self.query.weight\n",
      "LMRec.bert.encoder.layer.11.attention.self.query.bias\n",
      "LMRec.bert.encoder.layer.11.attention.self.key.weight\n",
      "LMRec.bert.encoder.layer.11.attention.self.key.bias\n",
      "LMRec.bert.encoder.layer.11.attention.self.value.weight\n",
      "LMRec.bert.encoder.layer.11.attention.self.value.bias\n",
      "LMRec.bert.encoder.layer.11.attention.output.dense.weight\n",
      "LMRec.bert.encoder.layer.11.attention.output.dense.bias\n",
      "LMRec.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "LMRec.bert.encoder.layer.11.intermediate.dense.weight\n",
      "LMRec.bert.encoder.layer.11.intermediate.dense.bias\n",
      "LMRec.bert.encoder.layer.11.output.dense.weight\n",
      "LMRec.bert.encoder.layer.11.output.dense.bias\n",
      "LMRec.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "LMRec.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "LMRec.bert.pooler.dense.weight\n",
      "LMRec.bert.pooler.dense.bias\n",
      "LMRec.classifier.0.weight\n",
      "LMRec.classifier.0.bias\n",
      "LMRec.classifier.2.weight\n",
      "LMRec.classifier.2.bias\n",
      "mitigator.0.weight\n",
      "mitigator.0.bias\n",
      "mitigator.2.weight\n",
      "mitigator.2.bias\n"
     ]
    }
   ],
   "source": [
    "state_dict = model_mit.state_dict()\n",
    "\n",
    "# Get parameter names\n",
    "parameter_names = state_dict.keys()\n",
    "\n",
    "# Print parameter names\n",
    "for name in parameter_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
