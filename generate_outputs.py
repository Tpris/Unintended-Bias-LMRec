# -*- coding: utf-8 -*-
"""Generate_outputs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10JqRa9RVa73R6BpMq5T5Lp1lzDH9Vyxo
"""

import argparse
import os
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm

from utils.utils import get_input_list
from utils.model_utils import BERT_classifier, Bert_patch_mitigator
from utils.mit_utils import load_weight_mitigator, get_sup

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default='data/Yelp_cities')
    parser.add_argument('--city_name', type=str, default='Atlanta')
    parser.add_argument('--topk', type=str, default=20, help='top number of recommendations to retrieve')
    parser.add_argument('--test_neutralization', action='store_true', help='whether to perform test-side neutralization towards the results')
    parser.add_argument('--use_tpu', action='store_true', help='whether to use tpu')
    parser.add_argument('--model_dir_root', type=str, default='models/{}/model.pt')
    parser.add_argument('--model_mit_dir_root', type=str, default='models/{}/model_mit.pt')
    parser.add_argument('--label_dir_root', type=str, default='data/Yelp_cities/{}_trainValidTest/')
    parser.add_argument('--biasAnalysis_path', type=str, default='data/bias_analysis/yelp/')
    p = parser.parse_args()

    # Get model and label directories
    model_dir = p.model_dir_root.format(p.city_name)
    model_mit_dir = p.model_mit_dir_root.format(p.city_name)
    label_dir = p.label_dir_root.format(p.city_name)
    print('Model directory', model_dir)
    print('Label directory', label_dir)

    # Set variable to True if the model use a patch
    MITIGATOR_PATCH = True
    # Set variable to True if the model mitigate bias without patch
    MITIGATOR = False

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # device = "cpu"

    # Get input sentence path
    input_path = p.biasAnalysis_path + 'input_sentences/'

    # Set up save path
    if p.test_neutralization:
        save_path = p.biasAnalysis_path + '{}_output_dataframes_Neutralized/'.format(p.city_name)
    else:
        save_path = p.biasAnalysis_path + '{}_output_dataframes/'.format(p.city_name)
    if not os.path.exists(save_path):
        os.mkdir(save_path)
    print('saving to:', save_path)

    max_len = 512

    # load data frame
    city_df = pd.read_csv('{}/{}_reviews.csv'.format(p.data_dir, p.city_name), lineterminator='\n')
    city_df.rename(columns={'stars': 'review_stars', 'text': 'review_text'}, inplace=True)

    df_map = city_df[['business_id', 'name']].drop_duplicates()
    busNumId_2_movieName = df_map.set_index('business_id').T.to_dict('records')[0]

    df_map = city_df[['business_id', 'categories']].drop_duplicates()
    busNumId_2_category = df_map.set_index('business_id').T.to_dict('records')[0]

    # calculate average
    df_map = city_df[['business_id', 'review_stars']].groupby('business_id').mean().reset_index()
    busNumId_2_avgStars = df_map.set_index('business_id').T.to_dict('records')[0]

    df_map = city_df[['business_id', 'price']].drop_duplicates()
    busNumId_2_price = df_map.set_index('business_id').T.to_dict('records')[0]

    """Load model and labels"""
    df = pd.read_csv('data/Yelp_cities/'+p.city_name+'_reviews.csv', lineterminator='\n')
    df = get_sup(df,2)
    # apply good types
    df.price = df.price.astype(float).fillna(0.0)
    df.loc['review_date'] = pd.to_datetime(df['review_date'])
    # filter data
    df = df.loc[(df['review_date'] >= '2008-01-01') & (df['review_date'] <= '2020-01-01')]
    df = get_sup(df,100)
    labels = list(df['business_id'].unique())

    loaded_model = BERT_classifier(len(labels)).to(device)
    if not MITIGATOR:
        loaded_model.load_state_dict(torch.load(model_dir))
    else:
        loaded_model.load_state_dict(torch.load(model_mit_dir))

    if MITIGATOR_PATCH:
        loaded_model = Bert_patch_mitigator(loaded_model).to(device)
        loaded_model = load_weight_mitigator(loaded_model, model_mit_dir)

    loaded_model.eval()
    print(loaded_model)

    for filename in os.listdir(input_path):
        print(filename)
        if filename.endswith(".csv"):
            current_df = pd.read_csv(input_path + filename)
            current_bais = filename.split('.csv')[0]
            qa_df = pd.DataFrame()

            pred_list = ()
            list_sent = current_df['input_sentence'].to_list()
            print(len(list_sent))
            # Step is used for reducing batch memory given to the GPU (GPU memory issue)
            STEP = 200
            for i in tqdm(range(0, len(list_sent), STEP)):
                max = i+STEP if i+STEP <len(list_sent) else len(list_sent)
                input = get_input_list(list_sent[i:max], max_len, device)
                pred = loaded_model(input)
                pred_list += (pred.cpu().detach().numpy(),)

            pred = np.concatenate(pred_list)
            print(pred.shape)
            # Retreive the k top recommendations
            sorted_prediction = np.argsort(-pred)[:, :p.topk]
            restaurant_ids_list = [np.array(labels)[pred] for pred in sorted_prediction]

            # Arrangement of information to be saved
            for index, row in current_df.iterrows():
                text_input = row['input_sentence']
                for rank, restaurant_id in enumerate(restaurant_ids_list[index]):
                    row["recommended_item"] = busNumId_2_movieName[restaurant_id]
                    row["categories"] = busNumId_2_category[restaurant_id]
                    row["avg_stars"] = round(busNumId_2_avgStars[restaurant_id], 2)
                    row["price"] = busNumId_2_price[restaurant_id]
                    row['rank'] = rank
                    if p.test_neutralization:
                        row['input_sentence'] = text_input

                    qa_df = pd.concat([qa_df, row.to_frame().T])
                    
                if index > 200 and index % 200 == 0:
                    qa_df.to_csv(save_path + '/yelp_qa_' + current_bais + '.csv')
            qa_df.to_csv(save_path + '/yelp_qa_' + current_bais + '.csv')
        else:
            continue
